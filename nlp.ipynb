{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68dd0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and load the spacy model\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "\n",
    "nlp=spacy.blank(\"en\") \n",
    "\n",
    "# Getting the ner component\n",
    "ner=nlp.add_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22830f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New label to add\n",
    "LABEL = \"FOOD\"\n",
    "\n",
    "# Training examples in the required format\n",
    "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\n",
    "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
    "              (\"Yogurt is a healthy type of food\", {\"entities\": [(0,6, \"FOOD\")]}),\n",
    "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
    "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e46b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chocolate soufflé is extremely famous french cuisine'\n",
      "  {'entities': [(0, 17, 'FOOD')]}]\n",
      " ['Burgers are the most commonly consumed fastfood'\n",
      "  {'entities': [(0, 7, 'FOOD')]}]\n",
      " ['Lasagna is another classic of Italy' {'entities': [(0, 7, 'FOOD')]}]\n",
      " ['Shrimps are famous in China too' {'entities': [(0, 7, 'FOOD')]}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omer.davarci\\PycharmProjects\\MetabolysmAnalysisAPI\\venv\\lib\\site-packages\\spacy\\training\\iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"China's noodles are very famous\" with entities \"[(8, 14, 'FOOD')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<thinc.optimizers.Optimizer at 0x1a0fd5f7ea0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "import numpy as np\n",
    "rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=57)\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for train_index, test_index in rs.split(TRAIN_DATA):\n",
    "    train = np.array(TRAIN_DATA)[train_index.astype(int)]\n",
    "    test = np.array(TRAIN_DATA)[test_index.astype(int)]\n",
    "\n",
    "print(test)\n",
    "    \n",
    "examples = []\n",
    "for text, annots in train:\n",
    "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "nlp.initialize(lambda: examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47c5706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53505594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 2.999999850988388}\n",
      "Losses {'ner': 9.333915024995804}\n",
      "Losses {'ner': 15.568691402673721}\n",
      "Losses {'ner': 23.427887231111526}\n",
      "Losses {'ner': 27.751150585711002}\n",
      "Losses {'ner': 31.096242882311344}\n",
      "Losses {'ner': 37.45279733091593}\n",
      "Losses {'ner': 42.975265838205814}\n",
      "Losses {'ner': 46.76392171531916}\n",
      "Losses {'ner': 51.8276744261384}\n",
      "Losses {'ner': 56.42733583599329}\n",
      "Losses {'ner': 5.521767973899841}\n",
      "Losses {'ner': 9.56801849603653}\n",
      "Losses {'ner': 11.900128066539764}\n",
      "Losses {'ner': 15.747126191854477}\n",
      "Losses {'ner': 17.828722268342972}\n",
      "Losses {'ner': 20.951374396681786}\n",
      "Losses {'ner': 23.465959936380386}\n",
      "Losses {'ner': 25.671693846583366}\n",
      "Losses {'ner': 27.235261999070644}\n",
      "Losses {'ner': 28.616341162472963}\n",
      "Losses {'ner': 30.169143456965685}\n",
      "Losses {'ner': 1.7895366502925754}\n",
      "Losses {'ner': 3.6105298111215234}\n",
      "Losses {'ner': 5.16975699365139}\n",
      "Losses {'ner': 6.813375455560163}\n",
      "Losses {'ner': 7.025237644062145}\n",
      "Losses {'ner': 8.781821798969759}\n",
      "Losses {'ner': 9.996981482709089}\n",
      "Losses {'ner': 11.681162245735322}\n",
      "Losses {'ner': 12.971609311887732}\n",
      "Losses {'ner': 14.424489475463815}\n",
      "Losses {'ner': 15.31858839247161}\n",
      "Losses {'ner': 1.2095442908321274}\n",
      "Losses {'ner': 2.778957172290802}\n",
      "Losses {'ner': 3.786268358320058}\n",
      "Losses {'ner': 5.37123356113193}\n",
      "Losses {'ner': 6.027225563605725}\n",
      "Losses {'ner': 7.595025670799487}\n",
      "Losses {'ner': 8.158913114134748}\n",
      "Losses {'ner': 9.115372364542026}\n",
      "Losses {'ner': 9.124628963661849}\n",
      "Losses {'ner': 9.797367281564107}\n",
      "Losses {'ner': 10.612141648501506}\n",
      "Losses {'ner': 0.9270349864714262}\n",
      "Losses {'ner': 1.3902487136654145}\n",
      "Losses {'ner': 3.155020390889831}\n",
      "Losses {'ner': 3.268879147630969}\n",
      "Losses {'ner': 3.404428307507331}\n",
      "Losses {'ner': 4.218312508835538}\n",
      "Losses {'ner': 4.393754956301677}\n",
      "Losses {'ner': 4.4039844891603686}\n",
      "Losses {'ner': 5.730865433231404}\n",
      "Losses {'ner': 5.741726570925923}\n",
      "Losses {'ner': 5.783714420421979}\n",
      "Losses {'ner': 0.011307139417121026}\n",
      "Losses {'ner': 0.01173362785769172}\n",
      "Losses {'ner': 0.17502872146590953}\n",
      "Losses {'ner': 0.18667606116285507}\n",
      "Losses {'ner': 0.18707700470045419}\n",
      "Losses {'ner': 0.31966450691995374}\n",
      "Losses {'ner': 0.31971012648561437}\n",
      "Losses {'ner': 1.5227285374249244}\n",
      "Losses {'ner': 1.5238669712298214}\n",
      "Losses {'ner': 1.5240481416401996}\n",
      "Losses {'ner': 3.504525186830927}\n",
      "Losses {'ner': 1.9351930097299443e-07}\n",
      "Losses {'ner': 0.008823860377481216}\n",
      "Losses {'ner': 0.008839126902534574}\n",
      "Losses {'ner': 0.00885099031813965}\n",
      "Losses {'ner': 0.008964823731662762}\n",
      "Losses {'ner': 0.008965327167055595}\n",
      "Losses {'ner': 0.011395618025251719}\n",
      "Losses {'ner': 0.011395683402437616}\n",
      "Losses {'ner': 2.0044683130264977}\n",
      "Losses {'ner': 2.0044738438995227}\n",
      "Losses {'ner': 2.0044766744588896}\n",
      "Losses {'ner': 0.005079033915720887}\n",
      "Losses {'ner': 0.005079412509603287}\n",
      "Losses {'ner': 0.005081620134305426}\n",
      "Losses {'ner': 0.005081637130282247}\n",
      "Losses {'ner': 0.010503289078704282}\n",
      "Losses {'ner': 0.010503301683968607}\n",
      "Losses {'ner': 0.017354010991086063}\n",
      "Losses {'ner': 0.01737638920680619}\n",
      "Losses {'ner': 0.017384475735544597}\n",
      "Losses {'ner': 1.8904586536531156}\n",
      "Losses {'ner': 1.8904588280182442}\n",
      "Losses {'ner': 4.701035134483022e-05}\n",
      "Losses {'ner': 0.011869659630270412}\n",
      "Losses {'ner': 0.011869768296927607}\n",
      "Losses {'ner': 0.011869791883459069}\n",
      "Losses {'ner': 1.1627767123651425}\n",
      "Losses {'ner': 1.1627767685715296}\n",
      "Losses {'ner': 1.1627767712208987}\n",
      "Losses {'ner': 1.1627768993094192}\n",
      "Losses {'ner': 1.162818244592819}\n",
      "Losses {'ner': 1.1628348479625954}\n",
      "Losses {'ner': 1.163133826741996}\n",
      "Losses {'ner': 1.0901359578034756e-07}\n",
      "Losses {'ner': 6.159718909912041e-07}\n",
      "Losses {'ner': 6.224270664371043e-07}\n",
      "Losses {'ner': 1.2981355454083732}\n",
      "Losses {'ner': 1.298135598722159}\n",
      "Losses {'ner': 1.298135736893372}\n",
      "Losses {'ner': 1.2981357496640138}\n",
      "Losses {'ner': 1.2981364505813404}\n",
      "Losses {'ner': 1.298137240206066}\n",
      "Losses {'ner': 1.2981457543714008}\n",
      "Losses {'ner': 1.2981465394412024}\n",
      "Losses {'ner': 3.801583148210591e-07}\n",
      "Losses {'ner': 0.00013758910505996412}\n",
      "Losses {'ner': 0.0001397300070570159}\n",
      "Losses {'ner': 0.0001413344162944112}\n",
      "Losses {'ner': 0.00014169894344576425}\n",
      "Losses {'ner': 0.00014169968378305265}\n",
      "Losses {'ner': 0.0001467225820425496}\n",
      "Losses {'ner': 0.0001467247281887791}\n",
      "Losses {'ner': 0.00014675693655928125}\n",
      "Losses {'ner': 0.00016864881325939684}\n",
      "Losses {'ner': 1.4721620952654213}\n",
      "Losses {'ner': 4.375525932160977e-09}\n",
      "Losses {'ner': 8.563891228437637e-09}\n",
      "Losses {'ner': 1.1451576457122264e-07}\n",
      "Losses {'ner': 3.0187804278807585e-07}\n",
      "Losses {'ner': 0.0017065732051560426}\n",
      "Losses {'ner': 0.0017134481333448961}\n",
      "Losses {'ner': 0.0017317925471934637}\n",
      "Losses {'ner': 0.0017318273321542904}\n",
      "Losses {'ner': 0.001732554479103486}\n",
      "Losses {'ner': 1.828299983005735}\n",
      "Losses {'ner': 1.828380413801179}\n",
      "Losses {'ner': 1.5552832946825727e-10}\n",
      "Losses {'ner': 0.31950088777709357}\n",
      "Losses {'ner': 0.31950231235972437}\n",
      "Losses {'ner': 0.3195023132852299}\n",
      "Losses {'ner': 0.31950231352403097}\n",
      "Losses {'ner': 0.3195024969830552}\n",
      "Losses {'ner': 0.31950254841941217}\n",
      "Losses {'ner': 0.31950273266459256}\n",
      "Losses {'ner': 0.3195027464746405}\n",
      "Losses {'ner': 2.3066060759727347}\n",
      "Losses {'ner': 2.3066060763761667}\n",
      "Losses {'ner': 6.767027594359911e-10}\n",
      "Losses {'ner': 7.324248946068637e-07}\n",
      "Losses {'ner': 7.324825273990167e-07}\n",
      "Losses {'ner': 7.459497276879279e-07}\n",
      "Losses {'ner': 8.799975264853208e-07}\n",
      "Losses {'ner': 8.833140470417016e-07}\n",
      "Losses {'ner': 1.9029141042830993}\n",
      "Losses {'ner': 1.9029143667171493}\n",
      "Losses {'ner': 1.902914426200522}\n",
      "Losses {'ner': 1.902921842890672}\n",
      "Losses {'ner': 1.9029307644584497}\n",
      "Losses {'ner': 6.704463804509484e-05}\n",
      "Losses {'ner': 1.2976422800498844}\n",
      "Losses {'ner': 1.297642288660655}\n",
      "Losses {'ner': 1.297643592290757}\n",
      "Losses {'ner': 1.2976437423395006}\n",
      "Losses {'ner': 1.2976438458056}\n",
      "Losses {'ner': 1.297645570060419}\n",
      "Losses {'ner': 1.2976504959328805}\n",
      "Losses {'ner': 1.2976643285956582}\n",
      "Losses {'ner': 1.2978189416502033}\n",
      "Losses {'ner': 1.297977431608471}\n",
      "Losses {'ner': 4.3524087064669424e-05}\n",
      "Losses {'ner': 8.238144785164071e-05}\n",
      "Losses {'ner': 0.0003483450370673811}\n",
      "Losses {'ner': 0.41005691906123226}\n",
      "Losses {'ner': 0.41006416693094094}\n",
      "Losses {'ner': 0.4100642712009136}\n",
      "Losses {'ner': 0.41319757001495655}\n",
      "Losses {'ner': 0.4131975700882412}\n",
      "Losses {'ner': 0.41398635794921673}\n",
      "Losses {'ner': 0.4139863579493377}\n",
      "Losses {'ner': 0.41398636312577836}\n",
      "Losses {'ner': 1.0515480077028156e-05}\n",
      "Losses {'ner': 7.047429620629055e-05}\n",
      "Losses {'ner': 7.047504577849401e-05}\n",
      "Losses {'ner': 7.047528304237496e-05}\n",
      "Losses {'ner': 7.048927518726213e-05}\n",
      "Losses {'ner': 7.048929237890682e-05}\n",
      "Losses {'ner': 0.9283872016718708}\n",
      "Losses {'ner': 0.9283872016932109}\n",
      "Losses {'ner': 0.9283872026511307}\n",
      "Losses {'ner': 0.9283872048240187}\n",
      "Losses {'ner': 0.928387206270759}\n",
      "Losses {'ner': 2.1009713585375965e-08}\n",
      "Losses {'ner': 2.2308535766557004e-08}\n",
      "Losses {'ner': 2.2404136122251048e-08}\n",
      "Losses {'ner': 2.117136070174588e-06}\n",
      "Losses {'ner': 0.8828429300729737}\n",
      "Losses {'ner': 0.8828429790551294}\n",
      "Losses {'ner': 0.8828429791245631}\n",
      "Losses {'ner': 0.8828429791276562}\n",
      "Losses {'ner': 0.8828484913204045}\n",
      "Losses {'ner': 0.8828485988768107}\n",
      "Losses {'ner': 0.8828485988838278}\n",
      "Losses {'ner': 4.526465294049901e-07}\n",
      "Losses {'ner': 4.537852774599476e-07}\n",
      "Losses {'ner': 4.5378534176944523e-07}\n",
      "Losses {'ner': 4.677176256661157e-07}\n",
      "Losses {'ner': 4.6776299839586415e-07}\n",
      "Losses {'ner': 6.060354686680554e-05}\n",
      "Losses {'ner': 6.061611279266536e-05}\n",
      "Losses {'ner': 9.338012777099358e-05}\n",
      "Losses {'ner': 0.003962651316145643}\n",
      "Losses {'ner': 0.003962651316319984}\n",
      "Losses {'ner': 0.003962652526901437}\n",
      "Losses {'ner': 8.710643300115244e-14}\n",
      "Losses {'ner': 0.0008892288905267081}\n",
      "Losses {'ner': 0.0008892289800479128}\n",
      "Losses {'ner': 0.0008892309310089322}\n",
      "Losses {'ner': 0.0008892313734896664}\n",
      "Losses {'ner': 0.0008892330128896501}\n",
      "Losses {'ner': 0.0008892333592514709}\n",
      "Losses {'ner': 0.000889865911284473}\n",
      "Losses {'ner': 0.0008899561921767696}\n",
      "Losses {'ner': 0.001459735938016949}\n",
      "Losses {'ner': 0.0014666557614554884}\n",
      "Losses {'ner': 1.3282510390880946e-08}\n",
      "Losses {'ner': 4.870169896406657e-05}\n",
      "Losses {'ner': 0.0003035912106716396}\n",
      "Losses {'ner': 0.0003035913195339222}\n",
      "Losses {'ner': 0.00030363938585759763}\n",
      "Losses {'ner': 0.00030402582246343913}\n",
      "Losses {'ner': 0.0003040466608845804}\n",
      "Losses {'ner': 0.00030404666142640137}\n",
      "Losses {'ner': 0.00030405234168040415}\n",
      "Losses {'ner': 0.00030405709949901986}\n",
      "Losses {'ner': 0.00030405715469505054}\n",
      "Losses {'ner': 3.654453641806513e-14}\n",
      "Losses {'ner': 8.529547118313019e-10}\n",
      "Losses {'ner': 0.0019474224668398089}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 0.0019474280931841212}\n",
      "Losses {'ner': 0.0019474282036954149}\n",
      "Losses {'ner': 0.0019474803775781702}\n",
      "Losses {'ner': 0.0019475452884119824}\n",
      "Losses {'ner': 0.0019475479956527046}\n",
      "Losses {'ner': 0.0019478687939710555}\n",
      "Losses {'ner': 0.0019478687949428118}\n",
      "Losses {'ner': 0.001947868843095954}\n",
      "Losses {'ner': 4.456039428223336e-15}\n",
      "Losses {'ner': 1.8024609830822888e-10}\n",
      "Losses {'ner': 2.008960936677982e-10}\n",
      "Losses {'ner': 9.550426058442257e-05}\n",
      "Losses {'ner': 9.550449745074715e-05}\n",
      "Losses {'ner': 9.55054563378578e-05}\n",
      "Losses {'ner': 9.550569354110242e-05}\n",
      "Losses {'ner': 9.55109151099435e-05}\n",
      "Losses {'ner': 9.702046123284951e-05}\n",
      "Losses {'ner': 9.712057823769158e-05}\n",
      "Losses {'ner': 9.71210626533859e-05}\n",
      "Losses {'ner': 9.926828928456256e-08}\n",
      "Losses {'ner': 1.0015722903426194e-07}\n",
      "Losses {'ner': 1.0015765566599365e-07}\n",
      "Losses {'ner': 5.177716314724843e-06}\n",
      "Losses {'ner': 5.177950928427357e-06}\n",
      "Losses {'ner': 5.236911726486621e-06}\n",
      "Losses {'ner': 5.2524014412241905e-06}\n",
      "Losses {'ner': 5.252407521972026e-06}\n",
      "Losses {'ner': 5.2524126841387564e-06}\n",
      "Losses {'ner': 5.252455357147202e-06}\n",
      "Losses {'ner': 5.2528314108288575e-06}\n",
      "Losses {'ner': 3.4969234318558564e-09}\n",
      "Losses {'ner': 3.6571715566256255e-09}\n",
      "Losses {'ner': 3.661204895090028e-09}\n",
      "Losses {'ner': 1.3015853635415269e-07}\n",
      "Losses {'ner': 1.5364187192546942e-07}\n",
      "Losses {'ner': 1.8181227205036508e-07}\n",
      "Losses {'ner': 1.3317260021337566e-06}\n",
      "Losses {'ner': 1.3317260326194721e-06}\n",
      "Losses {'ner': 1.3322803218843496e-06}\n",
      "Losses {'ner': 1.3368456579439947e-06}\n",
      "Losses {'ner': 3.7951901908959253e-06}\n",
      "Losses {'ner': 3.976453962967748e-14}\n",
      "Losses {'ner': 7.005017588885153e-11}\n",
      "Losses {'ner': 2.0309574880223443e-09}\n",
      "Losses {'ner': 3.398525601930825e-09}\n",
      "Losses {'ner': 3.737153646990417e-09}\n",
      "Losses {'ner': 6.758051107741378e-09}\n",
      "Losses {'ner': 7.2874070796480905e-09}\n",
      "Losses {'ner': 2.0701840107569606e-06}\n",
      "Losses {'ner': 2.0717282030319644e-06}\n",
      "Losses {'ner': 2.078020057260963e-06}\n",
      "Losses {'ner': 2.0780263341950716e-06}\n",
      "Losses {'ner': 1.1254623427032068e-12}\n",
      "Losses {'ner': 2.672507852967191e-10}\n",
      "Losses {'ner': 1.2891440213299718e-06}\n",
      "Losses {'ner': 1.3063839632311759e-06}\n",
      "Losses {'ner': 1.30902688167946e-06}\n",
      "Losses {'ner': 1.3098658903817265e-06}\n",
      "Losses {'ner': 1.3106547789805373e-06}\n",
      "Losses {'ner': 1.310663855222808e-06}\n",
      "Losses {'ner': 1.310673167612495e-06}\n",
      "Losses {'ner': 1.3106734099146374e-06}\n",
      "Losses {'ner': 1.310673862936848e-06}\n",
      "Losses {'ner': 1.5132105023183536e-10}\n",
      "Losses {'ner': 1.5135989014719527e-10}\n",
      "Losses {'ner': 1.5251737657299576e-10}\n",
      "Losses {'ner': 1.2504052474157204e-06}\n",
      "Losses {'ner': 1.2522140050344973e-06}\n",
      "Losses {'ner': 2.449720153957426e-06}\n",
      "Losses {'ner': 2.501601132201196e-06}\n",
      "Losses {'ner': 2.502109963932894e-06}\n",
      "Losses {'ner': 2.5021467296660387e-06}\n",
      "Losses {'ner': 2.502158696028114e-06}\n",
      "Losses {'ner': 2.5021587000710026e-06}\n",
      "Losses {'ner': 1.0807901663243936e-07}\n",
      "Losses {'ner': 1.080833314434863e-07}\n",
      "Losses {'ner': 1.0826627860179776e-07}\n",
      "Losses {'ner': 1.0830133489047455e-07}\n",
      "Losses {'ner': 1.1757098742892262e-07}\n",
      "Losses {'ner': 1.175712403700503e-07}\n",
      "Losses {'ner': 1.1757125131280181e-07}\n",
      "Losses {'ner': 5.450058802605586e-07}\n",
      "Losses {'ner': 5.463558169440778e-07}\n",
      "Losses {'ner': 5.465699915209891e-07}\n",
      "Losses {'ner': 5.470868903024483e-07}\n",
      "Losses {'ner': 2.4182819052540228e-09}\n",
      "Losses {'ner': 2.91230585240613e-09}\n",
      "Losses {'ner': 3.0090535174726845e-09}\n",
      "Losses {'ner': 5.414345382099742e-09}\n",
      "Losses {'ner': 5.41481440522086e-09}\n",
      "Losses {'ner': 5.445846034908037e-09}\n",
      "Losses {'ner': 5.445870706409906e-09}\n",
      "Losses {'ner': 6.008621198418301e-07}\n",
      "Losses {'ner': 7.670145122761378e-07}\n",
      "Losses {'ner': 0.0008335019597778777}\n",
      "Losses {'ner': 0.0008335032997786392}\n"
     ]
    }
   ],
   "source": [
    "# Importing requirements\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    # Training for 30 iterations     \n",
    "    for itn in range(30):\n",
    "    # shuffle examples before training\n",
    "        random.shuffle(examples)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(examples, size=sizes)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            # Calling update() over the iteration\n",
    "            nlp.update(batch, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16cf4465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'Chocolate soufflé is extremely famous french cuisine'\n",
      "Chocolate\n",
      "{'entities': [(0, 17, 'FOOD')]}\n",
      "Entities in 'Burgers are the most commonly consumed fastfood'\n",
      "Burgers\n",
      "{'entities': [(0, 7, 'FOOD')]}\n",
      "Entities in 'Lasagna is another classic of Italy'\n",
      "Lasagna\n",
      "{'entities': [(0, 7, 'FOOD')]}\n",
      "Entities in 'Shrimps are famous in China too'\n",
      "Shrimps\n",
      "{'entities': [(0, 7, 'FOOD')]}\n"
     ]
    }
   ],
   "source": [
    "# Testing the NER\n",
    "\n",
    "for t in test:\n",
    "    test_text = t[0]\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent)\n",
    "        print(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3812b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]  # Matched span\n",
    "    sent = span.sent  # Sentence containing matched span\n",
    "    # Append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{\n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        \"end\": span.end_char - sent.start_char,\n",
    "        \"label\": \"MATCH\",\n",
    "    }]\n",
    "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns, on_match=collect_sents)\n",
    "\n",
    "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Serve visualization of sentences containing match with displaCy\n",
    "# set manual=True to make displaCy render straight from a dictionary\n",
    "# (if you're not running the code within a Jupyer environment, you can\n",
    "# use displacy.serve instead)\n",
    "displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
