 3/1: from articles.models import Article, Pathway
 3/2:
articles = Article.objects.filter(pathways__isnull=True)
[entry for entry in articles]
 3/3:
articles = Article.objects.filter(pathways__isnull=True)
[entry for entry in articles]
 3/4:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
from articles.models import Article, Pathway
from django.db.models import Exists
 3/5:
articles = Article.objects.filter(pathways__isnull=True)
[entry for entry in articles]
 5/1: from articles.models import Article, Pathway
 5/2:
articles = Article.objects.filter(pathways__isnull=True)
[entry for entry in articles]
 5/3:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
from articles.models import Article, Pathway
from django.db.models import Exists
 5/4:
articles = Article.objects.filter(pathways__isnull=True)
[entry for entry in articles]
 5/5:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
from articles.models import Article, Pathway
from django.db.models import Exists
 5/6:
articles = Article.objects.filter(pathways__isnull=True)
[entry for entry in articles]
 6/1:

# Import and load the spacy model
import spacy
from spacy.training import Example

nlp=spacy.blank("en") 

# Getting the ner component
ner=nlp.add_pipe('ner')
 6/2:

# New label to add
LABEL = "FOOD"

# Training examples in the required format
TRAIN_DATA =[ ("Pizza is a common fast food.", {"entities": [(0, 5, "FOOD")]}),
              ("Pasta is an italian recipe", {"entities": [(0, 5, "FOOD")]}),
              ("China's noodles are very famous", {"entities": [(8,14, "FOOD")]}),
              ("Shrimps are famous in China too", {"entities": [(0,7, "FOOD")]}),
              ("Lasagna is another classic of Italy", {"entities": [(0,7, "FOOD")]}),
              ("Sushi is extemely famous and expensive Japanese dish", {"entities": [(0,5, "FOOD")]}),
              ("Unagi is a famous seafood of Japan", {"entities": [(0,5, "FOOD")]}),
              ("Tempura , Soba are other famous dishes of Japan", {"entities": [(0,7, "FOOD")]}),
              ("Udon is a healthy type of noodles", {"entities": [(0,4, "ORG")]}),
              ("Yogurt is a healthy type of food", {"entities": [(0,6, "FOOD")]}),
              ("Chocolate soufflé is extremely famous french cuisine", {"entities": [(0,17, "FOOD")]}),
              ("Flamiche is french pastry", {"entities": [(0,8, "FOOD")]}),
              ("Burgers are the most commonly consumed fastfood", {"entities": [(0,7, "FOOD")]}),
              ("Burgers are the most commonly consumed fastfood", {"entities": [(0,7, "FOOD")]}),
              ("Frenchfries are considered too oily", {"entities": [(0,11, "FOOD")]})
           ]
 6/3:

from sklearn.model_selection import ShuffleSplit
import numpy as np
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=57)

train = []
test = []

for train_index, test_index in rs.split(TRAIN_DATA):
    train = np.array(TRAIN_DATA)[train_index.astype(int)]
    test = np.array(TRAIN_DATA)[test_index.astype(int)]

print(test)
    
examples = []
for text, annots in train:
    examples.append(Example.from_dict(nlp.make_doc(text), annots))
nlp.initialize(lambda: examples)
 6/4:

# Add the new label to ner
ner.add_label(LABEL)

# Resume training
optimizer = nlp.resume_training()
move_names = list(ner.move_names)

# List of pipes you want to train
pipe_exceptions = ["ner", "trf_wordpiecer", "trf_tok2vec"]

# List of pipes which should remain unaffected in training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
 6/5:

# Importing requirements
from spacy.util import minibatch, compounding
import random

# Begin training by disabling other pipeline components
with nlp.disable_pipes(*other_pipes) :

    sizes = compounding(1.0, 4.0, 1.001)
    # Training for 30 iterations     
    for itn in range(30):
    # shuffle examples before training
        random.shuffle(examples)
        # batch up the examples using spaCy's minibatch
        batches = minibatch(examples, size=sizes)
        losses = {}
        for batch in batches:
            # Calling update() over the iteration
            nlp.update(batch, sgd=optimizer, drop=0.35, losses=losses)
            print("Losses", losses)
 6/6:

# Testing the NER

for t in test:
    test_text = t[0]
    doc = nlp(test_text)
    print("Entities in '%s'" % test_text)
    for ent in doc.ents:
        print(ent)
        print(t[1])
 6/7:

import spacy
from spacy.matcher import PhraseMatcher
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
matcher = PhraseMatcher(nlp.vocab)
matched_sents = []  # Collect data of matched sentences to be visualized
terms = ["Barack Obama", "Angela Merkel", "Washington, D.C."]

def collect_sents(matcher, doc, i, matches):
    match_id, start, end = matches[i]
    span = doc[start:end]  # Matched span
    sent = span.sent  # Sentence containing matched span
    # Append mock entity for match in displaCy style to matched_sents
    # get the match span by ofsetting the start and end of the span with the
    # start and end of the sentence in the doc
    match_ents = [{
        "start": span.start_char - sent.start_char,
        "end": span.end_char - sent.start_char,
        "label": "MATCH",
    }]
    matched_sents.append({"text": sent.text, "ents": match_ents})

patterns = [nlp.make_doc(text) for text in terms]
matcher.add("TerminologyList", patterns, on_match=collect_sents)

doc = nlp("German Chancellor Angela Merkel and US President Barack Obama "
          "converse in the Oval Office inside the White House in Washington, D.C.")
matches = matcher(doc)

# Serve visualization of sentences containing match with displaCy
# set manual=True to make displaCy render straight from a dictionary
# (if you're not running the code within a Jupyer environment, you can
# use displacy.serve instead)
displacy.render(matched_sents, style="ent", manual=True)
 6/8:

# Import and load the spacy model
import spacy
from spacy.training import Example

nlp=spacy.blank("en") 

# Getting the ner component
ner=nlp.add_pipe('ner')
 6/9:

# New label to add
LABEL = "FOOD"

# Training examples in the required format
TRAIN_DATA =[ ("Pizza is a common fast food.", {"entities": [(0, 5, "FOOD")]}),
              ("Pasta is an italian recipe", {"entities": [(0, 5, "FOOD")]}),
              ("China's noodles are very famous", {"entities": [(8,14, "FOOD")]}),
              ("Shrimps are famous in China too", {"entities": [(0,7, "FOOD")]}),
              ("Lasagna is another classic of Italy", {"entities": [(0,7, "FOOD")]}),
              ("Sushi is extemely famous and expensive Japanese dish", {"entities": [(0,5, "FOOD")]}),
              ("Unagi is a famous seafood of Japan", {"entities": [(0,5, "FOOD")]}),
              ("Tempura , Soba are other famous dishes of Japan", {"entities": [(0,7, "FOOD")]}),
              ("Udon is a healthy type of noodles", {"entities": [(0,4, "ORG")]}),
              ("Yogurt is a healthy type of food", {"entities": [(0,6, "FOOD")]}),
              ("Chocolate soufflé is extremely famous french cuisine", {"entities": [(0,17, "FOOD")]}),
              ("Flamiche is french pastry", {"entities": [(0,8, "FOOD")]}),
              ("Burgers are the most commonly consumed fastfood", {"entities": [(0,7, "FOOD")]}),
              ("Burgers are the most commonly consumed fastfood", {"entities": [(0,7, "FOOD")]}),
              ("Frenchfries are considered too oily", {"entities": [(0,11, "FOOD")]})
           ]
6/10:

from sklearn.model_selection import ShuffleSplit
import numpy as np
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=57)

train = []
test = []

for train_index, test_index in rs.split(TRAIN_DATA):
    train = np.array(TRAIN_DATA)[train_index.astype(int)]
    test = np.array(TRAIN_DATA)[test_index.astype(int)]

print(test)
    
examples = []
for text, annots in train:
    examples.append(Example.from_dict(nlp.make_doc(text), annots))
nlp.initialize(lambda: examples)
6/11:

# New label to add
LABEL = "FOOD"

# Training examples in the required format
TRAIN_DATA =[ ("Pizza is a common fast food.", {"entities": [(0, 5, "FOOD")]}),
              ("Pasta is an italian recipe", {"entities": [(0, 5, "FOOD")]}),
              ("China's noodles are very famous", {"entities": [(8,14, "FOOD")]}),
              ("Shrimps are famous in China too", {"entities": [(0,7, "FOOD")]}),
              ("Lasagna is another classic of Italy", {"entities": [(0,7, "FOOD")]}),
              ("Sushi is extemely famous and expensive Japanese dish", {"entities": [(0,5, "FOOD")]}),
              ("Unagi is a famous seafood of Japan", {"entities": [(0,5, "FOOD")]}),
              ("Tempura , Soba are other famous dishes of Japan", {"entities": [(0,7, "FOOD")]}),
              ("Udon is a healthy type of noodles", {"entities": [(0,4, "ORG")]}),
              ("Yogurt is a healthy type of food", {"entities": [(0,6, "FOOD")]}),
              ("Chocolate soufflé is extremely famous french cuisine", {"entities": [(0,17, "FOOD")]}),
              ("Flamiche is french pastry", {"entities": [(0,8, "FOOD")]}),
              ("Burgers are the most commonly consumed fastfood", {"entities": [(0,7, "FOOD")]}),
              ("Burgers are the most commonly consumed fastfood", {"entities": [(0,7, "FOOD")]}),
              ("Frenchfries are considered too oily", {"entities": [(0,11, "FOOD")]})
           ]
6/12:

from sklearn.model_selection import ShuffleSplit
import numpy as np
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=57)

train = []
test = []

for train_index, test_index in rs.split(TRAIN_DATA):
    train = np.array(TRAIN_DATA)[train_index.astype(int)]
    test = np.array(TRAIN_DATA)[test_index.astype(int)]

print(test)
    
examples = []
for text, annots in train:
    examples.append(Example.from_dict(nlp.make_doc(text), annots))
nlp.initialize(lambda: examples)
6/13:

from sklearn.model_selection import ShuffleSplit
import numpy as np
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=57)

train = []
test = []

for train_index, test_index in rs.split(TRAIN_DATA):
    train = np.array(TRAIN_DATA)[train_index.astype(int)]
    test = np.array(TRAIN_DATA)[test_index.astype(int)]

print(test)
    
examples = []
for text, annots in train:
    examples.append(Example.from_dict(nlp.make_doc(text), annots))
nlp.initialize(lambda: examples)
6/14:

from sklearn.model_selection import ShuffleSplit
import numpy as np
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=57)

train = []
test = []

for train_index, test_index in rs.split(TRAIN_DATA):
    train = np.array(TRAIN_DATA)[train_index.astype(int)]
    test = np.array(TRAIN_DATA)[test_index.astype(int)]

print(test)
    
examples = []
for text, annots in train:
    examples.append(Example.from_dict(nlp.make_doc(text), annots))
nlp.initialize(lambda: examples)
6/15:

from sklearn.model_selection import ShuffleSplit
import numpy as np
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=57)

train = []
test = []

for train_index, test_index in rs.split(TRAIN_DATA):
    train = np.array(TRAIN_DATA)[train_index.astype(int)]
    test = np.array(TRAIN_DATA)[test_index.astype(int)]

print(test)
    
examples = []
for text, annots in train:
    examples.append(Example.from_dict(nlp.make_doc(text), annots))
nlp.initialize(lambda: examples)
6/16:

from sklearn.model_selection import ShuffleSplit
import numpy as np
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=57)

train = []
test = []

for train_index, test_index in rs.split(TRAIN_DATA):
    train = np.array(TRAIN_DATA)[train_index.astype(int)]
    test = np.array(TRAIN_DATA)[test_index.astype(int)]

print(test)
    
examples = []
for text, annots in train:
    examples.append(Example.from_dict(nlp.make_doc(text), annots))
nlp.initialize(lambda: examples)
6/17:

# Add the new label to ner
ner.add_label(LABEL)

# Resume training
optimizer = nlp.resume_training()
move_names = list(ner.move_names)

# List of pipes you want to train
pipe_exceptions = ["ner", "trf_wordpiecer", "trf_tok2vec"]

# List of pipes which should remain unaffected in training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]
6/18:

# Importing requirements
from spacy.util import minibatch, compounding
import random

# Begin training by disabling other pipeline components
with nlp.disable_pipes(*other_pipes) :

    sizes = compounding(1.0, 4.0, 1.001)
    # Training for 30 iterations     
    for itn in range(30):
    # shuffle examples before training
        random.shuffle(examples)
        # batch up the examples using spaCy's minibatch
        batches = minibatch(examples, size=sizes)
        losses = {}
        for batch in batches:
            # Calling update() over the iteration
            nlp.update(batch, sgd=optimizer, drop=0.35, losses=losses)
            print("Losses", losses)
6/19:

# Importing requirements
from spacy.util import minibatch, compounding
import random

# Begin training by disabling other pipeline components
with nlp.disable_pipes(*other_pipes) :

    sizes = compounding(1.0, 4.0, 1.001)
    # Training for 30 iterations     
    for itn in range(30):
    # shuffle examples before training
        random.shuffle(examples)
        # batch up the examples using spaCy's minibatch
        batches = minibatch(examples, size=sizes)
        losses = {}
        for batch in batches:
            # Calling update() over the iteration
            nlp.update(batch, sgd=optimizer, drop=0.35, losses=losses)
            print("Losses", losses)
6/20:

# Importing requirements
from spacy.util import minibatch, compounding
import random

# Begin training by disabling other pipeline components
with nlp.disable_pipes(*other_pipes) :

    sizes = compounding(1.0, 4.0, 1.001)
    # Training for 30 iterations     
    for itn in range(30):
    # shuffle examples before training
        random.shuffle(examples)
        # batch up the examples using spaCy's minibatch
        batches = minibatch(examples, size=sizes)
        losses = {}
        for batch in batches:
            # Calling update() over the iteration
            nlp.update(batch, sgd=optimizer, drop=0.35, losses=losses)
            print("Losses", losses)
6/21:

# Testing the NER

for t in test:
    test_text = t[0]
    doc = nlp(test_text)
    print("Entities in '%s'" % test_text)
    for ent in doc.ents:
        print(ent)
        print(t[1])
6/22:

# Testing the NER

for t in test:
    test_text = t[0]
    doc = nlp(test_text)
    print("Entities in '%s'" % test_text)
    for ent in doc.ents:
        print(ent)
        print(t[1])
 7/1:
print('PyDev console: using IPython 7.22.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
import django; print('Django %s' % django.get_version())
sys.path.extend(['/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI', '/Applications/PyCharm.app/Contents/plugins/python/helpers/pycharm', '/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev'])
if 'setup' in dir(django): django.setup()
import django_manage_shell; django_manage_shell.run("/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI")
 8/1:
from datetime import datetime
from elasticsearch import Elasticsearch
 8/2:
from datetime import datetime
from elasticsearch import Elasticsearch
 8/3: es = Elasticsearch()
 8/4:
doc = {
    'author': 'kimchy',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=1, body=doc)
print(res['result'])
 8/5:
doc = {
    'author': 'kimchy',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=1, body=doc)
print(res['result'])
 8/6:
res = es.get(index="test-index", id=1)
print(res['_source'])
 8/7:
res = es.get(index="test-index", id=1)
print(res['_source'])
 8/8:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
 8/9:
doc = {
    'author': 'ben',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=1, body=doc)
print(res['result'])
8/10:
res = es.get(index="test-index", id=2)
print(res['_source'])
8/11:
doc = {
    'author': 'ben',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=1, body=doc)
print(res['result'])
8/12:
res = es.get(index="test-index", id=2)
print(res['_source'])
8/13:
doc = {
    'author': 'ben',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=2, body=doc)
print(res['result'])
8/14:
res = es.get(index="test-index", id=2)
print(res['_source'])
8/15:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
8/16:
doc = {
    'author': 'kim',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=1, body=doc)
print(res['result'])
8/17:
res = es.get(index="test-index", id=2)
print(res['_source'])
8/18:
doc = {
    'id': 1
    'author': 'kim',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", body=doc)
print(res['result'])
8/19:
doc = {
    'id': 1,
    'author': 'kim',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", body=doc)
print(res['result'])
8/20:
res = es.get(index="test-index", id=2)
print(res['_source'])
8/21:
res = es.get(index="test-index", id=1)
print(res['_source'])
8/22:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
8/23:
doc = {
    'id': 1
    'author': 'kim',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=doc.id, body=doc)
print(res['result'])
8/24:
doc = {
    'id': 1,
    'author': 'kim',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=doc.id, body=doc)
print(res['result'])
8/25:
doc = {
    'id': 1,
    'author': 'kim',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=doc['id'], body=doc)
print(res['result'])
8/26:
res = es.get(index="test-index", id=1)
print(res['_source'])
8/27:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
8/28:
doc = {
    'id': 1,
    'author': 'kim1',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=doc['id'], body=doc)
print(res['result'])
8/29:
doc = {
    'id': 2,
    'author': 'kim2',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=doc['id'], body=doc)
print(res['result'])
8/30:
doc = {
    'id': 3,
    'author': 'kim3',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=doc['id'], body=doc)
print(res['result'])
8/31:
res = es.get(index="test-index", id=1)
print(res['_source'])
8/32:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
8/33:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print("%(id) %(timestamp)s %(author)s: %(text)s" % hit["_source"])
8/34:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
8/35:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print(hit["_id"])
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
8/36:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print(hit["_id"])
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
8/37: es.indices.delete(index='test-index', ignore=[400, 404])
8/38:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print(hit["_id"])
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
8/39:
doc = {
    'id': 1,
    'author': 'kim1',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=doc['id'], body=doc)
print(res['result'])
8/40:
res = es.get(index="test-index", id=1)
print(res['_source'])
8/41:
doc = {
    'id': 1,
    'author': 'kim1',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=doc['id'], body=doc)
print(res['result'])
8/42:
res = es.get(index="test-index", id=1)
print(res['_source'])
8/43:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print(hit["_id"])
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
8/44: es.indices.delete(index='test-index', ignore=[400, 404])
8/45:
mport os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
8/46:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
8/47:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
8/48:
from articles.models import Article, Pathway
from django.db.models import Exists
8/49:
from articles.models import Article, Pathway
from django.db.models import Exists
8/50:
has_pathway = Article.objects.all()
[entry for entry in has_pathway]
10/1:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
10/2:
from articles.models import Article, Pathway
from django.db.models import Exists
10/3: has_pathway = Article.objects.raw('SELECT * from articles_article')
10/4: has_pathway = Article.objects.raw('SELECT * from articles_article')
10/5: [entry for enrty in has_pathway]
10/6: [entry for entry in has_pathway]
10/7: [entry for entry in has_pathway]
10/8: has_pathway[0]
10/9: has_pathway[0].id
10/10: has_pathway[1].id
10/11:
for a in has_pathway:
    print(a.id)
11/1:
from datetime import datetime
from elasticsearch import Elasticsearch
11/2:
from datetime import datetime
from elasticsearch import Elasticsearch
11/3: es = Elasticsearch()
11/4:
doc = {
    'id': 1,
    'author': 'kim1',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=doc['id'], body=doc)
print(res['result'])
11/5:
res = es.get(index="test-index", id=1)
print(res['_source'])
11/6:
es.indices.refresh(index="test-index")

res = es.search(index="test-index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print(hit["_id"])
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
11/7: es.indices.delete(index='test-index', ignore=[400, 404])
11/8:
doc = {
    'id': 1,
    'author': 'John Doe',
    'text': 'Abstract text is here!',
    'timestamp': datetime.now(),
}
res = es.index(index="test-index", id=doc['id'], body=doc)
print(res['result'])
14/1:
import scispacy
import spacy

nlp = spacy.load("en_ner_bc5cdr_md")
14/2:
import scispacy
import spacy

nlp = spacy.load("en_ner_bc5cdr_md")
14/3:
import scispacy
import spacy

nlp = spacy.load("en_ner_bc5cdr_md")
14/4:
from spacy import displacy
text = """
Myeloid derived suppressor cells (MDSC) are immature 
myeloid cells with immunosuppressive activity. 
They accumulate in tumor-bearing mice and humans 
with different types of cancer, including hepatocellular 
carcinoma (HCC).
"""
doc = nlp(text)
displacy.render(next(doc.ents), style='dep', jupyter=True)
14/5:
from spacy import displacy
text = """
Myeloid derived suppressor cells (MDSC) are immature 
myeloid cells with immunosuppressive activity. 
They accumulate in tumor-bearing mice and humans 
with different types of cancer, including hepatocellular 
carcinoma (HCC).
"""
doc = nlp(text)

displacy.render(doc.ents, style='dep', jupyter=True)
14/6:
from spacy import displacy
text = """
Myeloid derived suppressor cells (MDSC) are immature 
myeloid cells with immunosuppressive activity. 
They accumulate in tumor-bearing mice and humans 
with different types of cancer, including hepatocellular 
carcinoma (HCC).
"""
doc = nlp(text)

displacy.render(doc.ner, style='dep', jupyter=True)
14/7:
from spacy import displacy
text = """
Myeloid derived suppressor cells (MDSC) are immature 
myeloid cells with immunosuppressive activity. 
They accumulate in tumor-bearing mice and humans 
with different types of cancer, including hepatocellular 
carcinoma (HCC).
"""
doc = nlp(text)

displacy.render(doc.ents, style="ent", manual=True)
14/8:
from spacy import displacy
text = """
Myeloid derived suppressor cells (MDSC) are immature 
myeloid cells with immunosuppressive activity. 
They accumulate in tumor-bearing mice and humans 
with different types of cancer, including hepatocellular 
carcinoma (HCC).
"""
doc = nlp(text)

displacy.render(doc, style="ent", manual=True)
14/9:
from spacy import displacy
text = """
Myeloid derived suppressor cells (MDSC) are immature 
myeloid cells with immunosuppressive activity. 
They accumulate in tumor-bearing mice and humans 
with different types of cancer, including hepatocellular 
carcinoma (HCC).
"""
doc = nlp(text)

print(doc.ents)
14/10:
from spacy import displacy
text = """
Myeloid derived suppressor cells (MDSC) are immature 
myeloid cells with immunosuppressive activity. 
They accumulate in tumor-bearing mice and humans 
with different types of cancer, including hepatocellular 
carcinoma (HCC).
"""
doc = nlp(text)
14/11: displacy.render(doc, style="ent", manual=True)
14/12: displacy.render(doc.ents, style="ent", manual=True)
14/13: displacy.serve(doc, style='ent')
14/14: displacy.render(doc, style='ent')
14/15: displacy.render(doc, style='ent')
14/16: displacy.render(doc, style='ent')
14/17: displacy.render(doc, style='ent', jupyter=True)
19/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
from nltk.sentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
import requests

sia = SentimentIntensityAnalyzer()
19/2:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
from nltk.sentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
import requests

sia = SentimentIntensityAnalyzer()
19/3:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
19/4:
output_dir=Path('/content/')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
nlp.add_pipe('sentencizer')
19/5:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
nlp.add_pipe('sentencizer')
19/6:
import json

with open('data.json') as f:
    data = json.load(f)
19/7: print(data[0])
19/8: test_indexes = [150,406,513,101,535,700,814,809,395,31,196,484,390,676,465,642,285,60,77,299,685,14,267,145,666,55,868,344,816,474,872,770,876,432,790,622,308,181,337,318,877,538,279,630,97,304,836,706,626,692,8,144,483,580,270,542,230,363,266,301,215,306,520,391,34,609,668,764,27,30,582,732,760,384,271,878,372,283,293,202,317,806,526,758,742,824,527,672,568,586,695,874,236,720,787,382,590,475,204,506,253,456,331,379,716,358,258,252,428,638,5,717,457,103,434,247,808,342,826,272,142,403,796,686,519,776,113,727,249,40,338,214,158,175,567,563,839,447,718,141,667,511,62,79,491,351,840,263,345,530,356,546,477,505,85,604,75,239,772,316,364,829,1,413,873,689,828,794,710,813,140,319,362,367,264,320,466,222,200,162,587,632,460,726,416,54,45,172,255,193,471,409,39,614,533,18,557,156,231,641,50,843,748,640,122,576,167,784,627,859,10,105,68,240,310,698,261,412,602,545]
19/9:
examples = []
i = 0
for text, annots in data:
    if i in test_indexes:
        examples.append(Example.from_dict(nlp.make_doc(text), annots))
    i += 1
19/10:
results = nlp.evaluate(examples)
print(results)
19/11:
z = 0
for d in data:
    if z in test_indexes:
        doc = nlp(d[0])
        print("#######################")
        print("Annotated:")
        print(d[1]['entities'])
        print("\n\n\n#######################")
        print("Found from model:")
        print(doc.ents)
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
        html = displacy.render(doc, style="ent")
        input('Press ENTER to continue...')
        clear_output(wait=True)
    z += 1
19/12: text = """Glycolysis is the metabolic pathway that converts glucose C6H12O6, into pyruvic acid, CH3COCOOH. The free energy released in this process is used to form the high-energy molecules adenosine triphosphate (ATP) and reduced nicotinamide adenine dinucleotide (NADH).[1][2] Glycolysis is a sequence of ten reactions catalyzed by enzymes."""
19/13: doc = nlp(text)
19/14: html = displacy.render(doc, style="ent")
19/15:
doc = nlp(text)
print(doc.ents)
19/16: html = displacy.render(doc, style="ent")
22/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
23/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
23/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
nlp.add_pipe('sentencizer')
24/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
24/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
nlp.add_pipe('sentencizer')
24/3:
mport csv

with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        print(row)
25/1:
import csv

with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        print(row)
25/2:
import csv

with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            print(row)
        line_count += 1
25/3:
import csv

with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            print(row[0])
        line_count += 1
25/4:
import csv

patterns = []

with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            patterns.add(row[0])
        line_count += 1
25/5:
import csv

patterns = []

with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            patterns.append(row[0])
        line_count += 1
25/6:
import csv

patterns = []

with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            patterns.append(row[0])
        line_count += 1
25/7: patterns
25/8:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
25/9:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
nlp.add_pipe('sentencizer')
25/10:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
nlp.add_pipe('sentencizer')
ruler = nlp.add_pipe("entity_ruler")
25/11:
import csv

patterns = []

with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            obj = {"label": "PATHWAY", "pattern": row[0]}
            patterns.append(obj)
        line_count += 1
25/12: patterns
25/13:
import csv

patterns = []

with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            keys = []
            doc = nlp(row[0])

            for token in doc:
                keys.append(token.text)
                print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
                        token.shape_, token.is_alpha, token.is_stop)
            obj = {"label": "PATHWAY", "pattern": keys}
            patterns.append(obj)
        line_count += 1
25/14:
import csv

patterns = []

with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            keys = []
            doc = nlp(row[0])

            for token in doc:
                keys.append(token.text)
            obj = {"label": "PATHWAY", "pattern": keys}
            patterns.append(obj)
        line_count += 1
25/15: patterns
25/16:
import csv

patterns = []

nlp2 = spacy.load("en_core_web_sm")


with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            keys = []
            doc = nlp2(row[0])

            for token in doc:
                keys.append(token.text)
            obj = {"label": "PATHWAY", "pattern": keys}
            patterns.append(obj)
        line_count += 1
25/17:
import csv

patterns = []

nlp2 = spacy.load("en_core_web_sm")


with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            keys = []
            doc = nlp2(row[0])

            for token in doc:
                keys.append(token.text)
            obj = {"label": "PATHWAY", "pattern": keys}
            patterns.append(obj)
        line_count += 1
25/18:
import csv

patterns = []

nlp2 = spacy.load("en_core_web_sm")


with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            keys = []
            doc = nlp2(row[0])

            for token in doc:
                if token.tag_ != "punct":
                    keys.append({"LOWER": token.text})
            obj = {"label": "PATHWAY", "pattern": keys}
            patterns.append(obj)
        line_count += 1
25/19: patterns
25/20:
import csv

patterns = []

nlp2 = spacy.load("en_core_web_sm")


with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            keys = []
            doc = nlp2(row[0])

            for token in doc:
                print(token.text, token.tag_)
                if token.tag_ != "punct":
                    keys.append({"LOWER": token.text})
            obj = {"label": "PATHWAY", "pattern": keys}
            patterns.append(obj)
        line_count += 1
25/21:
import csv

patterns = []

nlp2 = spacy.load("en_core_web_sm")


with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            keys = []
            doc = nlp2(row[0])

            for token in doc:
                if token.pos_ != "PUNCT":
                    keys.append({"LOWER": token.text})
            obj = {"label": "PATHWAY", "pattern": keys}
            patterns.append(obj)
        line_count += 1
25/22: patterns
25/23:
import csv

patterns = []

nlp2 = spacy.load("en_core_web_sm")


with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            keys = []
            doc = nlp2(row[0])

            for token in doc:
                if token.pos_ != "PUNCT":
                    keys.append({"LOWER": token.text})
            obj = {"label": "PATHWAY", "pattern": keys}
            patterns.append(obj)
        line_count += 1

ruler.add_patterns(patterns)
25/24: doc = nlp("Glycolysis is the metabolic pathway that converts glucose C6H12O6, into pyruvic acid, CH3COCOOH. The free energy released in this process is used to form the high-energy molecules adenosine triphosphate (ATP) and reduced nicotinamide adenine dinucleotide (NADH).[1][2] Glycolysis is a sequence of ten reactions catalyzed by enzymes.")
25/25: doc = nlp("Glycolysis is the metabolic pathway that converts glucose C6H12O6, into pyruvic acid, CH3COCOOH. The free energy released in this process is used to form the high-energy molecules adenosine triphosphate (ATP) and reduced nicotinamide adenine dinucleotide (NADH).[1][2] Glycolysis is a sequence of ten reactions catalyzed by enzymes.")
25/26: html = displacy.render(doc, style="ent")
25/27: patterns
25/28: doc = nlp("Vitamin D metabolism is the metabolic pathway that converts glucose C6H12O6, into pyruvic acid, CH3COCOOH. The free energy released in this process is used to form the high-energy molecules adenosine triphosphate (ATP) and reduced nicotinamide adenine dinucleotide (NADH).[1][2] Glycolysis is a sequence of ten reactions catalyzed by enzymes.")
25/29: html = displacy.render(doc, style="ent")
25/30:
doc = nlp("""Purine and Pyrimidine Metabolism
Antonio Blanco, Gustavo Blanco, in Medical Biochemistry, 2017

Pyrimidine biosynthesis
Similar to purine synthesis, pyrimidine bases are formed from relatively simple precursors (aspartate and carbamoyl). Fig. 18.5 shows the contribution of both of these compounds to the structure of the pyrimidine ring.

""")
25/31: html = displacy.render(doc, style="ent")
25/32:
import json

with open('data.json') as f:
    data = json.load(f)
25/33: print(data[0])
25/34: test_indexes = [150,406,513,101,535,700,814,809,395,31,196,484,390,676,465,642,285,60,77,299,685,14,267,145,666,55,868,344,816,474,872,770,876,432,790,622,308,181,337,318,877,538,279,630,97,304,836,706,626,692,8,144,483,580,270,542,230,363,266,301,215,306,520,391,34,609,668,764,27,30,582,732,760,384,271,878,372,283,293,202,317,806,526,758,742,824,527,672,568,586,695,874,236,720,787,382,590,475,204,506,253,456,331,379,716,358,258,252,428,638,5,717,457,103,434,247,808,342,826,272,142,403,796,686,519,776,113,727,249,40,338,214,158,175,567,563,839,447,718,141,667,511,62,79,491,351,840,263,345,530,356,546,477,505,85,604,75,239,772,316,364,829,1,413,873,689,828,794,710,813,140,319,362,367,264,320,466,222,200,162,587,632,460,726,416,54,45,172,255,193,471,409,39,614,533,18,557,156,231,641,50,843,748,640,122,576,167,784,627,859,10,105,68,240,310,698,261,412,602,545]
25/35:
examples = []
i = 0
for text, annots in data:
    if i in test_indexes:
        examples.append(Example.from_dict(nlp.make_doc(text), annots))
    i += 1
25/36:
results = nlp.evaluate(examples)
print(results)
25/37:
z = 0
for d in data:
    if z in test_indexes:
        doc = nlp(d[0])
        print("#######################")
        print("Annotated:")
        print(d[1]['entities'])
        print("\n\n\n#######################")
        print("Found from model:")
        print(doc.ents)
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
        html = displacy.render(doc, style="ent")
        input('Press ENTER to continue...')
        clear_output(wait=True)
    z += 1
25/38:
doc = nlp("""Purine and Pyrimidine Metabolism
Antonio Blanco, Gustavo Blanco, in Medical Biochemistry, 2017

Pyrimidine biosynthesis
Similar to purine synthesis, pyrimidine bases are formed from relatively simple precursors (aspartate and carbamoyl). Fig. 18.5 shows the contribution of both of these compounds to the structure of the pyrimidine ring.

""")
25/39: html = displacy.render(doc, style="ent")
30/1:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
30/2: from articles.models import Article, Pathway
30/3:
all_articles = Article.objects.all()[:10]
[ar for ar in all_articles]
30/4:
all_pathways = Pathway.objects.all()
[ar for ar in all_pathways]
30/5:
i = 0
for ar in all_articles:
    abstractText = ar.abstract_text.lower()
    for me in all_pathways:
        nameOfPathway = me.name.lower()
        if nameOfPathway != "":
            if nameOfPathway in abstractText:
                ar.pathways.add(me)
    i += 1
    print(ar.id)
    if i == 10:
        break
31/1:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re

from articles.models import Article, Pathway
from django.db.models import Exists

has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
31/2:
import scispacy
import spacy

nlp = spacy.load("en_ner_bc5cdr_md")
31/3: from spacy import displacy
31/4:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    print("\n\n\n#######################")
    print("Found from model:")
    print(doc.ents)
    html = displacy.render(doc, style="ent", jupyter=True)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
31/5:
import scispacy
import spacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
nlp = spacy.load("en_ner_bc5cdr_md")
31/6: from spacy import displacy
31/7:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    print("\n\n\n#######################")
    print("Found from model:")
    print(doc.ents)
    html = displacy.render(doc, style="ent", jupyter=True)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
31/8:
from spacy import displacy

a = Article.objects.all()[0]
print(a)
31/9:
from spacy import displacy

a = Article.objects.all()[0]
print(a)
html = displacy.render(nlp(a), style="ent", jupyter=True)
31/10:
from spacy import displacy

a = Article.objects.all()[0]
print(a)
html = displacy.render(nlp(a.abstract_text), style="ent", jupyter=True)
40/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
40/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
nlp.add_pipe('sentencizer')
ruler = nlp.add_pipe("entity_ruler")
40/3:
import csv

patterns = []

nlp2 = spacy.load("en_core_web_sm")


with open('pathways.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count != 0:
            keys = []
            doc = nlp2(row[0])

            for token in doc:
                if token.pos_ != "PUNCT":
                    keys.append({"LOWER": token.text})
            obj = {"label": "PATHWAY", "pattern": keys}
            patterns.append(obj)
        line_count += 1

ruler.add_patterns(patterns)
40/4: nlp.to_dist("./content_nlp")
40/5:
from pathlib import Path
output_dir=Path('/content2/')

# Saving the model to the output directory
if not output_dir.exists():
    output_dir.mkdir()
nlp.meta['name'] = 'pathway_ner_model2'  # rename model
nlp.to_disk(output_dir)
print("Saved model to", output_dir)
40/6:
from pathlib import Path
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Saving the model to the output directory
if not output_dir.exists():
    output_dir.mkdir()
nlp.meta['name'] = 'pathway_ner_model2'  # rename model
nlp.to_disk(output_dir)
print("Saved model to", output_dir)
40/7:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
40/8:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
40/9:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
40/10:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re

from articles.models import Article, Pathway
from django.db.models import Exists

has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
40/11:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re

from articles.models import Article, Pathway
from django.db.models import Exists

has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
40/12:
from spacy import displacy

a = Article.objects.all()[0]
print(a)
html = displacy.render(nlp(a.abstract_text), style="ent", jupyter=True)
31/11: from spacy import displacy
31/12:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    print("\n\n\n#######################")
    print("Found from model:")
    print(doc.ents)
    html = displacy.render(doc, style="ent", jupyter=True)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
40/13:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    print("\n\n\n#######################")
    print("Found from model:")
    print(doc.ents)
    html = displacy.render(doc, style="ent", jupyter=True)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
42/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
42/3: nlp_disease = spacy.load("en_ner_bc5cdr_md")
42/4:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re

from articles.models import Article, Pathway
from django.db.models import Exists

has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
42/5:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
42/6:
from nltk.sentiment import SentimentIntensityAnalyzer

sia = SentimentIntensityAnalyzer()
42/7:
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download(["vader_lexicon"])
sia = SentimentIntensityAnalyzer()
42/8:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download(["vader_lexicon"])

sia = SentimentIntensityAnalyzer()
42/9:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
42/10:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abtract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia(entity.sent))
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/11:
has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
42/12:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia(entity.sent))
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/13:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent))
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/14:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/15:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent))
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/16:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
42/17:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent))
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/18:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(type(entity.sent))
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/19:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(type(entity.sent.text))
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/20:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/21:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
    if len(doc_disease.ents) > 0:
        print(doc_disease.ents)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/22:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
    if len(doc_disease.ents) > 0:
        print(doc_disease.ents)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/23:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
42/24:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
42/25:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
    if len(doc_disease.ents) > 0:
        print(doc_disease.ents)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/26:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).classify)
    if len(doc_disease.ents) > 0:
        print(doc_disease.ents)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/27:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).classify())
    if len(doc_disease.ents) > 0:
        print(doc_disease.ents)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
42/28:
!pip3 install flair
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
s = flair.data.Sentence(sentence)
flair_sentiment.predict(s)
total_sentiment = s.labels
total_sentiment
42/29:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
s = flair.data.Sentence(sentence)
flair_sentiment.predict(s)
total_sentiment = s.labels
total_sentiment
42/30:
!pip3 install ipywidgets
!pip3 install flair
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
s = flair.data.Sentence(sentence)
flair_sentiment.predict(s)
total_sentiment = s.labels
total_sentiment
42/31: !pip3 install ipywidgets
42/32:
!pip3 install flair
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
s = flair.data.Sentence(sentence)
flair_sentiment.predict(s)
total_sentiment = s.labels
total_sentiment
47/1:
!pip3 install flair
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
s = flair.data.Sentence(sentence)
flair_sentiment.predict(s)
total_sentiment = s.labels
total_sentiment
47/2:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
47/3:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
47/4:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
47/5: nlp_disease = spacy.load("en_ner_bc5cdr_md")
47/6:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
47/7:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
47/8:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
#s = flair.data.Sentence(sentence)
#flair_sentiment.predict(s)
#total_sentiment = s.labels
#total_sentiment
47/9:
has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
47/10:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).classify())
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            print(s.labels)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
47/11:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            print(s.labels)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
47/12:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            print(s.labels)
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/13:
z = 0
for d in has_pathway:
    print(d.url)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            print(s.labels)
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/14:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            print(s.labels)
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/15:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    print(disease)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/16:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    print(disease.tag)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/17:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    print(disease.tag_)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/18:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    print(disease.label)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/19:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    print(disease.label)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/20:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia.polarity_scores(entity.sent.text))
            print(TextBlob(entity.sent.text).sentiment)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    print(disease.label_)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/21:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/22:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score.compound)
            print(textblob_score)
            print(flair_score)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/23:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score['compound'])
            print(textblob_score)
            print(flair_score)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/24:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score['compound'])
            print(textblob_score.polarity)
            print(flair_score)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/25:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score['compound'])
            print(textblob_score.polarity)
            print(flair_score.value, flair_score.score)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/26:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score['compound'])
            print(textblob_score.polarity)
            print(flair_score[0].value, flair_score[0].score)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/27:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        rel = PathwayDiseaseRelation(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
            
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/28:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
47/29:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        rel = PathwayDiseaseRelation(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
                        rel.save()
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/30:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        rel = PathwayDiseaseRelation(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
                        rel.save()
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
47/31:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        rel = PathwayDiseaseRelation(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
                        print(rel)
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
49/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
49/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
49/3: nlp_disease = spacy.load("en_ner_bc5cdr_md")
49/4:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
49/5:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
49/6:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
#s = flair.data.Sentence(sentence)
#flair_sentiment.predict(s)
#total_sentiment = s.labels
#total_sentiment
49/7:
has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
49/8:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        rel = PathwayDiseaseRelation(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
                        rel.save()
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
49/9:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        rel = PathwayDiseaseRelation.objects.get_or_create(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
                        rel.save()
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
49/10:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        rel = PathwayDiseaseRelation.objects.get_or_create(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
            input('Press ENTER to continue...')
            clear_output(wait=True)
    z += 1
49/11:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        rel = PathwayDiseaseRelation.objects.get_or_create(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
                        print(rel[0], rel[1])
    z += 1
49/12:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        rel = PathwayDiseaseRelation.objects.get_or_create(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
    z += 1
52/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
52/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
52/3: nlp_disease = spacy.load("en_ner_bc5cdr_md")
52/4:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
52/5:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
52/6:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
#s = flair.data.Sentence(sentence)
#flair_sentiment.predict(s)
#total_sentiment = s.labels
#total_sentiment
52/7:
has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
52/8:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        rel = PathwayDiseaseRelation.objects.get_or_create(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
    z += 1
52/9:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        print(sia_score, flair_score, textblob_score)
                        rel = PathwayDiseaseRelation.objects.get_or_create(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sia_score=sia_score, textblob_score=textblob_score, flair_score=flair_score)
    z += 1
57/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
57/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
57/3: nlp_disease = spacy.load("en_ner_bc5cdr_md")
57/4:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
57/5:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
57/6:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
#s = flair.data.Sentence(sentence)
#flair_sentiment.predict(s)
#total_sentiment = s.labels
#total_sentiment
57/7:
has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
57/8:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        print(sia_score, flair_score, textblob_score)
                        obj, created = PathwayDiseaseRelation.objects.get_or_create(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sentence=entity.sent.text)
                        if created:
                            obj.sia_score = sia_score['compound']
                            obj.textblob_polarity = textblob_score.polarity
                            obj.textblob_subjectivity = textblob_score.subjectivity
                            obj.flair_score = flair_score[0].score
                            obj.save()
    z += 1
57/9: print(z)
59/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
59/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
59/3: nlp_disease = spacy.load("en_ner_bc5cdr_md")
59/4:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
59/5:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
59/6:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
#s = flair.data.Sentence(sentence)
#flair_sentiment.predict(s)
#total_sentiment = s.labels
#total_sentiment
59/7:
has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
59/8:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        print(sia_score, flair_score, textblob_score)
                        obj, created = PathwayDiseaseRelation.objects.get_or_create(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sentence=entity.sent.text)
                        if created:
                            obj.sia_score = sia_score['compound']
                            obj.textblob_polarity = textblob_score.polarity
                            obj.textblob_subjectivity = textblob_score.subjectivity
                            obj.flair_score = flair_score[0].score
                            obj.save()
    z += 1
59/9: print(z)
60/1:
import requests
from articles.models import Article, Author

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=cancer&resultType=core&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files)
60/2: response
60/3: response[0]
60/4: response.data
60/5: response
60/6: response[0]
60/7: response.text
60/8:
import requests
from articles.models import Article, Author

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=cancer&resultType=core&format=json&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files)
60/9: response.text
60/10: response.json()
60/11: response
60/12:
import requests
from articles.models import Article, Author

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=cancer&resultType=core&format=json&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files).json()
60/13: response
60/14:
import requests
from articles.models import Article, Author

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=glycolysis&resultType=core&format=json&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files).json()
60/15: response
60/16: response.resultList[0]
60/17: response['resultList'][0]
60/18: response['resultList']
60/19: response['resultList']['result']
60/20: response['resultList']['result'][0]
60/21:
import requests
from articles.models import Article, Author

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=glycolysispathway&resultType=core&format=json&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files).json()
60/22: response
60/23: response['resultList']['result'][0]
60/24:
import requests
from articles.models import Article, Author

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=glycolysis&resultType=core&format=json&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files).json()
60/25: response
60/26: response['resultList']['result'][0]
60/27:
import requests
from articles.models import Article, Author, Pathway

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=glycolysis&resultType=core&format=json&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files).json()
60/28: a = Pathway.get(name='glycolysis')
60/29: a = Pathway.objects.get(name='glycolysis')
60/30:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
60/31: a = Pathway.objects.get(name='glycolysis')
60/32:
for ar in response['resultList']['result']:
    print(ar.abstract_text)
    #article = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
60/33:
for ar in response['resultList']['result']:
    print(ar['abstract_text'])
    #article = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
60/34:
for ar in response['resultList']['result']:
    print(ar)
    #article = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
60/35:
response['resultList']['result'][0]

    #article = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
60/36:
for ar in response['resultList']['result']:
    print(ar['abstractText'])
60/37:
for m in response['resultList']['result']:
    article = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
    article.pathways.get_or_create(a)
60/38:
for m in response['resultList']['result']:
    article, created = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
    article.pathways.get_or_create(a)
60/39:
for m in response['resultList']['result']:
    article, created = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
    article.pathways.get_or_create(a)
60/40: a = Pathway.objects.get(name='glycolysis')
60/41:
a = Pathway.objects.get(name='glycolysis')
print(a)
60/42:
for m in response['resultList']['result']:
    article, created = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
    article.pathways.add(a)
60/43:
for m in response['resultList']['result']:
    try:
        article, created = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
        article.pathways.add(a)
    except e as Exception:
        print(e)
60/44:
for m in response['resultList']['result']:
    try:
        article, created = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
        article.pathways.add(a)
    except Exception as e:
        print(e)
59/10:
has_pathway = Article.objects.filter(pathways__name='glycolysis').all()
[entry for entry in has_pathway]
59/11:
has_pathway = Article.objects.all().exclude(pathways=None)
has_pathway = has_pathway.filter(pathways__name='glycolysis').all()
[entry for entry in has_pathway]
59/12:
has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
59/13:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
59/14:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
59/15: nlp_disease = spacy.load("en_ner_bc5cdr_md")
59/16:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
59/17:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
59/18:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
#s = flair.data.Sentence(sentence)
#flair_sentiment.predict(s)
#total_sentiment = s.labels
#total_sentiment
59/19:
has_pathway = Article.objects.all().exclude(pathways=None)
has_pathway = has_pathway.filter(pathways__name='glycolysis').all()
[entry for entry in has_pathway]
59/20:
has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
61/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
61/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
61/3: nlp_disease = spacy.load("en_ner_bc5cdr_md")
61/4:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
61/5:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
61/6:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
#s = flair.data.Sentence(sentence)
#flair_sentiment.predict(s)
#total_sentiment = s.labels
#total_sentiment
61/7:
has_pathway = Article.objects.filter(pathways__n)
has_pathway = has_pathway.filter(pathways__name='glycolysis').all()
[entry for entry in has_pathway]
61/8:
has_pathway = Article.objects.filter(pathways__n)
[entry for entry in has_pathway]
61/9:
has_pathway = Article.objects.exclude(pathways=None).all()
has_pathway = has_pathway.filter(pathways__name__exact='glycolysis').all()
[entry for entry in has_pathway]
61/10:
has_pathway = Article.objects.exclude(pathways=None).all()
has_pathway = has_pathway.filter(pathways__name__exact='glycolysis').all()
[entry.abstract_text for entry in has_pathway]
61/11:
has_pathway = Article.objects.exclude(pathways=None).all()
has_pathway = has_pathway.filter(pathways__name__exact='glycolysis').all()
[entry for entry in has_pathway]
61/12:
has_pathway = Article.objects.exclude(pathways=None).all()
has_pathway = has_pathway.filter(pathways__name__exact='glycolysis').all()
[entry for entry in has_pathway]

doc = nlp(has_pathway[0].abstract_text)
print(doct.ents)
61/13:
has_pathway = Article.objects.exclude(pathways=None).all()
has_pathway = has_pathway.filter(pathways__name__exact='glycolysis').all()
[entry for entry in has_pathway]

doc = nlp(has_pathway[0].abstract_text)
print(doc.ents)
62/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
62/2:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
62/3:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
nlp.add_pipe('sentencizer')
ruler = nlp.add_pipe("entity_ruler")
62/4:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
ruler = nlp.add_pipe("entity_ruler")
62/5:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
nlp.entity_ruler
62/6:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
62/7:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
ruler = nlp.get_pipe('entity_ruler')
62/8: ruler.add_patterns([{"label": "PATHWAY", "pattern": "glycolysis"}])
62/9: ruler.add_patterns([{"label": "PATHWAY", "pattern": "glycolysis"}])
62/10:
# Output directory
from pathlib import Path
output_dir=Path('/content2/')

# Saving the model to the output directory
if not output_dir.exists():
    output_dir.mkdir()
nlp.meta['name'] = 'pathway_ner_model'  # rename model
nlp.to_disk(output_dir)
print("Saved model to", output_dir)
62/11:
# Output directory
from pathlib import Path
output_dir=Path('/content3/')

# Saving the model to the output directory
if not output_dir.exists():
    output_dir.mkdir()
nlp.meta['name'] = 'pathway_ner_model'  # rename model
nlp.to_disk(output_dir)
print("Saved model to", output_dir)
62/12:
# Output directory
from pathlib import Path
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')
# Saving the model to the output directory
if not output_dir.exists():
    output_dir.mkdir()
nlp.meta['name'] = 'pathway_ner_model'  # rename model
nlp.to_disk(output_dir)
print("Saved model to", output_dir)
61/14:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
61/15:
has_pathway = Article.objects.exclude(pathways=None).all()
has_pathway = has_pathway.filter(pathways__name__exact='glycolysis').all()
[entry for entry in has_pathway]

print(nlp(has_pathway[0].abstract_text).ents)
61/16:
has_pathway = Article.objects.exclude(pathways=None).all()
has_pathway = has_pathway.filter(pathways__name__exact='glycolysis').all()
[entry for entry in has_pathway]
61/17:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        print(sia_score, flair_score, textblob_score)
                        obj, created = PathwayDiseaseRelation.objects.get_or_create(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sentence=entity.sent.text)
                        if created:
                            obj.sia_score = sia_score['compound']
                            obj.textblob_polarity = textblob_score.polarity
                            obj.textblob_subjectivity = textblob_score.subjectivity
                            obj.flair_score = flair_score[0].score
                            obj.save()
    z += 1
61/18:
s = flair.data.Sentence("The data indicated that TP decreased glucose consumption, lactate production, and the mRNA levels of glycolysis-related transporters and enzymes.")
flair_sentiment.predict(s)
flair_score = s.labels
61/19:
s = flair.data.Sentence("The data indicated that TP decreased glucose consumption, lactate production, and the mRNA levels of glycolysis-related transporters and enzymes.")
flair_sentiment.predict(s)
flair_score = s.labels
print(flair_score)
67/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
67/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
67/3: nlp_disease = spacy.load("en_ner_bc5cdr_md")
67/4:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
67/5:
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
67/6:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
#s = flair.data.Sentence(sentence)
#flair_sentiment.predict(s)
#total_sentiment = s.labels
#total_sentiment
73/1:
import requests
from articles.models import Article, Author, Pathway

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=urea%20cycle&resultType=core&format=json&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files).json()
73/2:
import requests
from articles.models import Article, Author, Pathway

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=urea%20cycle&resultType=core&format=json&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files).json()
print(response.text)
73/3:
import requests
from articles.models import Article, Author, Pathway

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=urea%20cycle&resultType=core&format=json&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files).json()
print(response)
73/4:
import requests
from articles.models import Article, Author, Pathway

url = "https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST?query=urea%20cycle&resultType=core&format=json&pageSize=100"

payload = {}
files = {}
headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.request("POST", url, headers=headers, data=payload, files=files).json()
73/5:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
73/6:
a = Pathway.objects.get(name='urea cycle')
print(a)
73/7:
a = Pathway.objects.get(name__icontains="urea")
print(a)
73/8:
a = Pathway.objects.get(name__icontains="urea")
print(a.name)
73/9:
for m in response['resultList']['result']:
    try:
        article, created = Article.objects.get_or_create(abstract_text=m['abstractText'], pub_date=m['firstPublicationDate'], name=m['title'], doi=m['doi'])
        article.pathways.add(a)
    except Exception as e:
        print(e)
67/7:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
67/8:
has_pathway = Article.objects.exclude(pathways=None).all()
has_pathway = has_pathway.filter(pathways__name__exact='Urea cycle').all()
[entry for entry in has_pathway]
67/9:
z = 0
for d in has_pathway:
    print(d.id)
    doc = nlp(d.abstract_text)
    doc_disease = nlp_disease(d.abstract_text)
        
    if len(doc.ents) > 0:
        for entity in doc.ents:
            sia_score = sia.polarity_scores(entity.sent.text)
            textblob_score = TextBlob(entity.sent.text).sentiment
            
            s = flair.data.Sentence(entity.sent.text)
            flair_sentiment.predict(s)
            flair_score = s.labels
            print("\n\n#######################")
            print('Entity extracted : ', entity.text)
            print("\n\n#######################")
            print('Sentence extracted from : ', entity.sent)
            print("\n\n#######################")
            print(sia_score)
            print(textblob_score)
            print(flair_score)
            
            if len(doc_disease.ents) > 0:
                for disease in doc_disease.ents:
                    if disease.label_ == 'DISEASE':
                        print(sia_score, flair_score, textblob_score)
                        obj, created = PathwayDiseaseRelation.objects.get_or_create(pathway_name=entity.text.lower(), disease_name=disease.text.lower(), article_id=d.id, sentence=entity.sent.text)
                        if created:
                            obj.sia_score = sia_score['compound']
                            obj.textblob_polarity = textblob_score.polarity
                            obj.textblob_subjectivity = textblob_score.subjectivity
                            obj.flair_score = flair_score[0].score
                            obj.save()
    z += 1
74/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
74/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
ruler = nlp.get_pipe('entity_ruler')
74/3:
import json

with open('data.json') as f:
    data = json.load(f)
74/4: print(data[0])
74/5: test_indexes = [150,406,513,101,535,700,814,809,395,31,196,484,390,676,465,642,285,60,77,299,685,14,267,145,666,55,868,344,816,474,872,770,876,432,790,622,308,181,337,318,877,538,279,630,97,304,836,706,626,692,8,144,483,580,270,542,230,363,266,301,215,306,520,391,34,609,668,764,27,30,582,732,760,384,271,878,372,283,293,202,317,806,526,758,742,824,527,672,568,586,695,874,236,720,787,382,590,475,204,506,253,456,331,379,716,358,258,252,428,638,5,717,457,103,434,247,808,342,826,272,142,403,796,686,519,776,113,727,249,40,338,214,158,175,567,563,839,447,718,141,667,511,62,79,491,351,840,263,345,530,356,546,477,505,85,604,75,239,772,316,364,829,1,413,873,689,828,794,710,813,140,319,362,367,264,320,466,222,200,162,587,632,460,726,416,54,45,172,255,193,471,409,39,614,533,18,557,156,231,641,50,843,748,640,122,576,167,784,627,859,10,105,68,240,310,698,261,412,602,545]
74/6:
examples = []
i = 0
for text, annots in data:
    if i in test_indexes:
        examples.append(Example.from_dict(nlp.make_doc(text), annots))
    i += 1
74/7:
results = nlp.evaluate(examples)
print(results)
74/8:
import json

with open('data.json') as f:
    data = json.load(f)
74/9: print(len(data))
76/1:
import json

with open('data.json', 'w') as json_file:
    json.dump(TRAIN_DATA, json_file)
76/2: TRAIN_DATA = []
76/3:
import json

with open('data.json', 'w') as json_file:
    json.dump(TRAIN_DATA, json_file)
76/4:
from sklearn.model_selection import ShuffleSplit
import numpy as np
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=0)

train = []
test = []

for train_index, test_index in rs.split(TRAIN_DATA):
    print(test_index)
    train = np.array(TRAIN_DATA)[train_index.astype(int)]
    test = np.array(TRAIN_DATA)[test_index.astype(int)]
76/5: TRAIN_DATA = []
76/6:
import json

with open('data.json', 'w') as json_file:
    json.dump(TRAIN_DATA, json_file)
76/7:
from sklearn.model_selection import ShuffleSplit
import numpy as np
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=0)

train = []
test = []

for train_index, test_index in rs.split(TRAIN_DATA):
    print(test_index)
    train = np.array(TRAIN_DATA)[train_index.astype(int)]
    test = np.array(TRAIN_DATA)[test_index.astype(int)]
74/10:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
74/11:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
ruler = nlp.get_pipe('entity_ruler')
74/12:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
74/13:
import json

with open('data.json') as f:
    data = json.load(f)
74/14: print(len(data))
74/15: test_indexes = [150,406,513,101,535,700,814,809,395,31,196,484,390,676,465,642,285,60,77,299,685,14,267,145,666,55,868,344,816,474,872,770,876,432,790,622,308,181,337,318,877,538,279,630,97,304,836,706,626,692,8,144,483,580,270,542,230,363,266,301,215,306,520,391,34,609,668,764,27,30,582,732,760,384,271,878,372,283,293,202,317,806,526,758,742,824,527,672,568,586,695,874,236,720,787,382,590,475,204,506,253,456,331,379,716,358,258,252,428,638,5,717,457,103,434,247,808,342,826,272,142,403,796,686,519,776,113,727,249,40,338,214,158,175,567,563,839,447,718,141,667,511,62,79,491,351,840,263,345,530,356,546,477,505,85,604,75,239,772,316,364,829,1,413,873,689,828,794,710,813,140,319,362,367,264,320,466,222,200,162,587,632,460,726,416,54,45,172,255,193,471,409,39,614,533,18,557,156,231,641,50,843,748,640,122,576,167,784,627,859,10,105,68,240,310,698,261,412,602,545]
74/16:
import json

with open('data.json') as f:
    data = json.load(f)
74/17: print(len(data))
74/18:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
74/19:
import json

with open('data.json') as f:
    data = json.load(f)
74/20: print(len(data))
74/21: test_indexes = [150,406,513,101,535,700,814,809,395,31,196,484,390,676,465,642,285,60,77,299,685,14,267,145,666,55,868,344,816,474,872,770,876,432,790,622,308,181,337,318,877,538,279,630,97,304,836,706,626,692,8,144,483,580,270,542,230,363,266,301,215,306,520,391,34,609,668,764,27,30,582,732,760,384,271,878,372,283,293,202,317,806,526,758,742,824,527,672,568,586,695,874,236,720,787,382,590,475,204,506,253,456,331,379,716,358,258,252,428,638,5,717,457,103,434,247,808,342,826,272,142,403,796,686,519,776,113,727,249,40,338,214,158,175,567,563,839,447,718,141,667,511,62,79,491,351,840,263,345,530,356,546,477,505,85,604,75,239,772,316,364,829,1,413,873,689,828,794,710,813,140,319,362,367,264,320,466,222,200,162,587,632,460,726,416,54,45,172,255,193,471,409,39,614,533,18,557,156,231,641,50,843,748,640,122,576,167,784,627,859,10,105,68,240,310,698,261,412,602,545]
74/22:
examples = []
i = 0
for text, annots in data:
    if i in test_indexes:
        examples.append(Example.from_dict(nlp.make_doc(text), annots))
    i += 1
74/23:
results = nlp.evaluate(examples)
print(results)
74/24:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content3')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
74/25:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
74/26:
results = nlp.evaluate(examples)
print(results)
74/27:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
74/28:
results = nlp.evaluate(examples)
print(results)
doc = nlp(data[676]['abstract_text'])
74/29: print(data[676])
74/30:
results = nlp.evaluate(examples)
print(results)
doc = nlp(data[676])
print(doc.ents)
74/31:
results = nlp.evaluate(examples)
print(results)
doc = nlp(data[676][0])
print(doc.ents)
74/32:
doc = nlp(data[512][0])
print(doc.ents)
74/33:
doc = nlp(data[700][0])
print(doc.ents)
74/34:
doc = nlp(data[700][0])
print(doc.ents)
print(data[700][1])
74/35:
doc = nlp(data[101][0])
print(doc.ents)
print(data[700][1])
74/36:
doc = nlp(data[77][0])
print(doc.ents)
print(data[700][1])
67/10:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
67/11:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
#s = flair.data.Sentence(sentence)
#flair_sentiment.predict(s)
#total_sentiment = s.labels
#total_sentiment
67/12:
data = PathwayDiseaseRelation.objects.all()
[entry for entry in data]
67/13:
data = PathwayDiseaseRelation.objects.all()
[entry for entry in data]
67/14:
data = PathwayDiseaseRelation.objects.all()
[entry for entry in data]
67/15:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
67/16:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
67/17:
data = PathwayDiseaseRelation.objects.all()
[entry for entry in data]
67/18:
d = PathwayDiseaseRelation.objects.all()
[entry for entry in d]
77/1:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
from IPython.display import clear_output

from articles.models import Article, Pathway, PathwayDiseaseRelation
from django.db.models import Exists
77/2:
has_pathway = Article.objects.exclude(pathways=None).all()
has_pathway = has_pathway.filter(pathways__name__exact='Urea cycle').all()
[entry for entry in has_pathway]
77/3:
d = PathwayDiseaseRelation.objects.all()
[entry for entry in d]
77/4:
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
#s = flair.data.Sentence(sentence)
#flair_sentiment.predict(s)
#total_sentiment = s.labels
#total_sentiment
77/5:
for p in d:
    s = flair.data.Sentence(p.sentence)
    flair_sentiment.predict(s)
    flair_score = s.labels
    print(flair_score[0].value)
77/6:
for p in d:
    s = flair.data.Sentence(p.sentence)
    flair_sentiment.predict(s)
    flair_score = s.labels
    if flair_score[0].value == 'NEGATIVE'
        p.flair_score = -1 * p.flair_score
        p.save()
77/7:
for p in d:
    s = flair.data.Sentence(p.sentence)
    flair_sentiment.predict(s)
    flair_score = s.labels
    if flair_score[0].value == 'NEGATIVE':
        p.flair_score = -1 * p.flair_score
        p.save()
78/1:
# import libraries
import pandas as pd
import seaborn as sns
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# import model
nltk.download('vader_lexicon')

# configure size of heatmap
sns.set(rc={'figure.figsize':(35,3)})

# function to visualize 
def visualize_sentiments(data):
  sns.heatmap(pd.DataFrame(data).set_index("Sentence").T,center=0, annot=True, cmap = "PiYG")

# text
sentence = "To inspire and guide entrepreneurs is where I get my joy of contribution"

# sentiment analysis
sid = SentimentIntensityAnalyzer()

# call method 
print(sid.polarity_scores(sentence))

# heatmap 
visualize_sentiments({
    "Sentence":["SENTENCE"] + sentence.split(),
    "Sentiment":[sid.polarity_scores(sentence)["compound"]] + [sid.polarity_scores(word)["compound"] for word in sentence.split()]
})
78/2: !pip3 install pandas
78/3: !pip3 install seaborn
78/4:
# import libraries
import pandas as pd
import seaborn as sns
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# import model
nltk.download('vader_lexicon')

# configure size of heatmap
sns.set(rc={'figure.figsize':(35,3)})

# function to visualize 
def visualize_sentiments(data):
  sns.heatmap(pd.DataFrame(data).set_index("Sentence").T,center=0, annot=True, cmap = "PiYG")

# text
sentence = "To inspire and guide entrepreneurs is where I get my joy of contribution"

# sentiment analysis
sid = SentimentIntensityAnalyzer()

# call method 
print(sid.polarity_scores(sentence))

# heatmap 
visualize_sentiments({
    "Sentence":["SENTENCE"] + sentence.split(),
    "Sentiment":[sid.polarity_scores(sentence)["compound"]] + [sid.polarity_scores(word)["compound"] for word in sentence.split()]
})
78/5:
# import libraries
import pandas as pd
import seaborn as sns
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# import model
nltk.download('vader_lexicon')

# configure size of heatmap
sns.set(rc={'figure.figsize':(35,3)})

# function to visualize 
def visualize_sentiments(data):
  sns.heatmap(pd.DataFrame(data).set_index("Sentence").T,center=0, annot=True, cmap = "PiYG")

# text
sentence = "Because glycolysis is the most important source of energy in erythrocytes and in some types of skeletal muscle fibres, inherited diseases of glycolysis are mainly characterized by haemolytic anaemia and/or metabolic myopathy."

# sentiment analysis
sid = SentimentIntensityAnalyzer()

# call method 
print(sid.polarity_scores(sentence))

# heatmap 
visualize_sentiments({
    "Sentence":["SENTENCE"] + sentence.split(),
    "Sentiment":[sid.polarity_scores(sentence)["compound"]] + [sid.polarity_scores(word)["compound"] for word in sentence.split()]
})
78/6:
# import libraries
import pandas as pd
import seaborn as sns
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# import model
nltk.download('vader_lexicon')

# configure size of heatmap
sns.set(rc={'figure.figsize':(35,3)})

# function to visualize 
def visualize_sentiments(data):
  sns.heatmap(pd.DataFrame(data).set_index("Sentence").T,center=0, annot=True, cmap = "PiYG")

# text
sentence = "Glycolysis in hepatocytes controls hepatic glucose production, and when glucose is overproduced by the liver without having a means of being broken down by the body, hyperglycemia results"

# sentiment analysis
sid = SentimentIntensityAnalyzer()

# call method 
print(sid.polarity_scores(sentence))

# heatmap 
visualize_sentiments({
    "Sentence":["SENTENCE"] + sentence.split(),
    "Sentiment":[sid.polarity_scores(sentence)["compound"]] + [sid.polarity_scores(word)["compound"] for word in sentence.split()]
})
78/7:
# import libraries
import pandas as pd
import seaborn as sns
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# import model
nltk.download('vader_lexicon')

# configure size of heatmap
sns.set(rc={'figure.figsize':(35,3)})

# function to visualize 
def visualize_sentiments(data):
  sns.heatmap(pd.DataFrame(data).set_index("Sentence").T,center=0, annot=True, cmap = "PiYG")

# text
sentence = "Cellular uptake of glucose occurs in response to insulin signals, and glucose is subsequently broken down through glycolysis, lowering blood sugar levels."

# sentiment analysis
sid = SentimentIntensityAnalyzer()

# call method 
print(sid.polarity_scores(sentence))

# heatmap 
visualize_sentiments({
    "Sentence":["SENTENCE"] + sentence.split(),
    "Sentiment":[sid.polarity_scores(sentence)["compound"]] + [sid.polarity_scores(word)["compound"] for word in sentence.split()]
})
78/8:
# import libraries
import pandas as pd
import seaborn as sns
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# import model
nltk.download('vader_lexicon')

# configure size of heatmap
sns.set(rc={'figure.figsize':(35,3)})

# function to visualize 
def visualize_sentiments(data):
  sns.heatmap(pd.DataFrame(data).set_index("Sentence").T,center=0, annot=True, cmap = "PiYG")

# text
sentence = "Glycolysis in hepatocytes controls hepatic glucose production, and when glucose is overproduced by the liver without having a means of being broken down by the body, hyperglycemia results."

# sentiment analysis
sid = SentimentIntensityAnalyzer()

# call method 
print(sid.polarity_scores(sentence))

# heatmap 
visualize_sentiments({
    "Sentence":["SENTENCE"] + sentence.split(),
    "Sentiment":[sid.polarity_scores(sentence)["compound"]] + [sid.polarity_scores(word)["compound"] for word in sentence.split()]
})
80/1:
print('PyDev console: using IPython 7.22.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
import django; print('Django %s' % django.get_version())
sys.path.extend(['/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI', '/Applications/PyCharm.app/Contents/plugins/python/helpers/pycharm', '/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev'])
if 'setup' in dir(django): django.setup()
import django_manage_shell; django_manage_shell.run("/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI")
83/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
83/2:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
83/3:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re

from articles.models import Article, Pathway
from django.db.models import Exists

has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
83/4:
z = 0
for d in has_pathway:
    doc = nlp(d.abstract_text)
    print("\n\n\n#######################")
    print("Found from model:")
    print(doc.ents)
    html = displacy.render(doc, style="ent", jupyter=True)
    input('Press ENTER to continue...')
    clear_output(wait=True)
    z += 1
83/5:
doc = nlp("Glycolysis is increasing in cancer.")
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
83/6:
doc = nlp("glycolysis is increasing in cancer.")
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
83/7:
from textblob import TextBlob
doc = nlp("glycolysis is increasing that. Cancer is bad.")
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob("glycolysis is increasing that")
83/8:
from textblob import TextBlob
doc = nlp("glycolysis is increasing that. Cancer is bad.")
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob("glycolysis is increasing that")
print(textblob_score)
83/9:
from textblob import TextBlob
doc = nlp("glycolysis is increasing that. Cancer is bad.")
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob("glycolysis is increasing that")
print(textblob_score.sentiment)
83/10:
from textblob import TextBlob
doc = nlp("glycolysis is increasing that. Cancer is bad.")
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob("glycolysis is increasing that. Cancer is bad.")
print(textblob_score.sentiment)
83/11:
from textblob import TextBlob
doc = nlp("glycolysis is increasing that. Cancer is bad.")
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob("glycolysis is increasing that.")
print(textblob_score.sentiment)
textblob_score = TextBlob("glycolysis is increasing that. Cancer is bad.")
print(textblob_score.sentiment)
86/1:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
86/2:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
86/3:
output_dir=Path('/Users/omerfaruk.davarci/Documents/MetabolysmAnalysisAPI/content2')

# Loading the model from the directory
print("Loading from", output_dir)
nlp = spacy.load(output_dir)
86/4:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re

from articles.models import Article, Pathway
from django.db.models import Exists

has_pathway = Article.objects.all().exclude(pathways=None)
[entry for entry in has_pathway]
86/5:
from textblob import TextBlob
doc = nlp("glycolysis is increasing that. Cancer is bad.")
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob("arginine is increasing that.")
print(textblob_score.sentiment)
textblob_score = TextBlob("arginine is increasing that. cancer is bad.")
print(textblob_score.sentiment)
86/6:
from textblob import TextBlob
doc = nlp("Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer.")
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob("arginine is increasing that.")
print(textblob_score.sentiment)
textblob_score = TextBlob("arginine is increasing that. cancer is bad.")
print(textblob_score.sentiment)
86/7:
from textblob import TextBlob
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
86/8:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
86/9:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print(sia_score)
86/10:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
86/11:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print(sia_score)
s = flair.data.Sentence(text)
print(flair_sentiment.predict(s))
86/12:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print(sia_score)
s = flair.data.Sentence(text)
flair_sentiment.predict(s)
print(s)
86/13:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print('##')
print(sia_score)
s = flair.data.Sentence(text)
print('##')
flair_sentiment.predict(s)
print(s)
86/14:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
print("\n\n\n#######################")
html = displacy.render(doc, style="ent", jupyter=True)
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
print("\n\n\n#######################")
sia_score = sia.polarity_scores(text)
print("\n\n\n#######################")
print(sia_score)
s = flair.data.Sentence(text)
print("\n\n\n#######################")
flair_sentiment.predict(s)
print(s)
86/15:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
print("\n\n\n#######################")
html = displacy.render(doc, style="ent", jupyter=True)
print("\n\n\n#######################")
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print("\n\n\n#######################")
print(sia_score)
s = flair.data.Sentence(text)
print("\n\n\n#######################")
flair_sentiment.predict(s)
print(s)
86/16:
# Output directory
import spacy
from pathlib import Path
from spacy.training import Example
from spacy import displacy
from IPython.core.display import display, HTML
from IPython.display import clear_output
import requests
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download([
    "names",
    "stopwords",
    "state_union",
    "averaged_perceptron_tagger",
    "vader_lexicon",
    "punkt",])
sia = SentimentIntensityAnalyzer()
from textblob import TextBlob
import flair
flair_sentiment = flair.models.TextClassifier.load('en-sentiment')
nlp_disease = spacy.load("en_ner_bc5cdr_md")
86/17:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
doc_disease = nlp_disease(text)
doc.ents += doc_disease.ents
print("\n\n\n#######################")
print("Found from model:")
print(doc.ents)
print("\n\n\n#######################")
html = displacy.render(doc, style="ent", jupyter=True)
print("\n\n\n#######################")
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print("\n\n\n#######################")
print(sia_score)
s = flair.data.Sentence(text)
print("\n\n\n#######################")
flair_sentiment.predict(s)
print(s)
86/18:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
doc_disease = nlp_disease(text)
print("\n\n\n#######################")
print("Found from models:")
print("Pathways: ",doc.ents)
print("Diseases: ",doc_disease.ents)
print("\n\n\n#######################")
html = displacy.render(doc, style="ent", jupyter=True)
print("\n\n\n#######################")
html = displacy.render(doc_disease, style="ent", jupyter=True)
print("\n\n\n#######################")
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print("\n\n\n#######################")
print(sia_score)
s = flair.data.Sentence(text)
print("\n\n\n#######################")
flair_sentiment.predict(s)
print(s)
86/19:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
doc_disease = nlp_disease(text)
ents = [ent for ent in list(doc.ents) if ent.label_ != 'DISEASE']
doc_disease.ents = ents
print("\n\n\n#######################")
print("Found from models:")
print("Pathways: ",doc.ents)
print("Diseases: ",doc_disease.ents)
print("\n\n\n#######################")
html = displacy.render(doc, style="ent", jupyter=True)
print("\n\n\n#######################")
html = displacy.render(doc_disease, style="ent", jupyter=True)
print("\n\n\n#######################")
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print("\n\n\n#######################")
print(sia_score)
s = flair.data.Sentence(text)
print("\n\n\n#######################")
flair_sentiment.predict(s)
print(s)
86/20:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
doc_disease = nlp_disease(text)
ents = [ent for ent in list(doc_disease.ents) if ent.label_ != 'DISEASE']
doc_disease.ents = ents
print("\n\n\n#######################")
print("Found from models:")
print("Pathways: ",doc.ents)
print("Diseases: ",doc_disease.ents)
print("\n\n\n#######################")
html = displacy.render(doc, style="ent", jupyter=True)
print("\n\n\n#######################")
html = displacy.render(doc_disease, style="ent", jupyter=True)
print("\n\n\n#######################")
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print("\n\n\n#######################")
print(sia_score)
s = flair.data.Sentence(text)
print("\n\n\n#######################")
flair_sentiment.predict(s)
print(s)
86/21:
text = "Untargeted analysis identified differences in sphingolipid metabolism, steroid biosynthesis, and arginine and proline metabolism in pre- and post-operative patients with colorectal cancer."
doc = nlp(text)
doc_disease = nlp_disease(text)
ents = [ent for ent in list(doc_disease.ents) if ent.label_ == 'DISEASE']
doc_disease.ents = ents
print("\n\n\n#######################")
print("Found from models:")
print("Pathways: ",doc.ents)
print("Diseases: ",doc_disease.ents)
print("\n\n\n#######################")
html = displacy.render(doc, style="ent", jupyter=True)
print("\n\n\n#######################")
html = displacy.render(doc_disease, style="ent", jupyter=True)
print("\n\n\n#######################")
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print("\n\n\n#######################")
print(sia_score)
s = flair.data.Sentence(text)
print("\n\n\n#######################")
flair_sentiment.predict(s)
print(s)
86/22:
text = "The results revealed that tumor cells located in a nutrient-enriched environment showed low to no sensitivity to metabolic inhibitors targeting glycolysis, fatty acid oxidation, or oxidative phosphorylation."
doc = nlp(text)
doc_disease = nlp_disease(text)
ents = [ent for ent in list(doc_disease.ents) if ent.label_ == 'DISEASE']
doc_disease.ents = ents
print("\n\n\n#######################")
print("Found from models:")
print("Pathways: ",doc.ents)
print("Diseases: ",doc_disease.ents)
print("\n\n\n#######################")
html = displacy.render(doc, style="ent", jupyter=True)
print("\n\n\n#######################")
html = displacy.render(doc_disease, style="ent", jupyter=True)
print("\n\n\n#######################")
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print("\n\n\n#######################")
print(sia_score)
s = flair.data.Sentence(text)
print("\n\n\n#######################")
flair_sentiment.predict(s)
print(s)
86/23:
text = "The results revealed that tumor cells located in a nutrient-enriched environment showed low to no sensitivity to metabolic inhibitors targeting glycolysis, fatty acid oxidation, or oxidative phosphorylation."
doc = nlp(text)
doc_disease = nlp_disease(text)
ents = [ent for ent in list(doc_disease.ents) if ent.label_ == 'DISEASE']
doc_disease.ents = ents
print("\n\n\n#######################")
print("Found from models:")
print("Pathways: ",doc.ents)
print("Diseases: ",doc_disease.ents)
print("\n\n\n#######################")
html = displacy.render(doc, style="ent", jupyter=True)
print("\n\n\n#######################")
html = displacy.render(doc_disease, style="ent", jupyter=True)
print("\n\n\n#######################")
textblob_score = TextBlob(text)
print(textblob_score.sentiment)
sia_score = sia.polarity_scores(text)
print("\n\n\n#######################")
print(sia_score)
s = flair.data.Sentence(text)
print("\n\n\n#######################")
flair_sentiment.predict(s)
print(s)
89/1:
data = [
    ['woof', 1],
    ['bark', 1],
    ['ruff', 1],
    ['bowwow', 1],
    ['roar', 0],
    ['bah', 0],
    ['meow', 0],
    ['ribbit', 0],
    ['moo', 0],
    ['yip', 0],
    ['pika', 0]
]
89/2:
X = []
y = []
for i in data:
    X.append( i[0] )
    y.append( i[1] )
89/3: X
89/4: y
89/5:
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
X_vectorized = vectorizer.fit_transform(X)
89/6: X_vectorized
89/7: print(X_vectorized)
89/8:
from sklearn.linear_model import LinearRegression
import numpy as np
regressor = LinearRegression()
regressor.fit(X_vectorized, y)
89/9:
test_feature = vectorizer.transform(['woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbit'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meoww'])
prediction = regressor.predict(test_feature)
print(prediction)
89/10:
test_feature = vectorizer.transform(['W00f'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbit'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meoww'])
prediction = regressor.predict(test_feature)
print(prediction)
89/11:
test_feature = vectorizer.transform(['WOof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbit'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meoww'])
prediction = regressor.predict(test_feature)
print(prediction)
89/12:
test_feature = vectorizer.transform(['WOoooof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbit'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meoww'])
prediction = regressor.predict(test_feature)
print(prediction)
89/13:
test_feature = vectorizer.transform(['woofwoof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbit'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meoww'])
prediction = regressor.predict(test_feature)
print(prediction)
89/14:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbit'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meoww'])
prediction = regressor.predict(test_feature)
print(prediction)
89/15:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbit'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meoww'])
prediction = regressor.predict(test_feature)
print(prediction)
89/16:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbit'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meo'])
prediction = regressor.predict(test_feature)
print(prediction)
89/17:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribwoofbit'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meo'])
prediction = regressor.predict(test_feature)
print(prediction)
89/18:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['barkand'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meo'])
prediction = regressor.predict(test_feature)
print(prediction)
89/19:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbitbarkand'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['meo'])
prediction = regressor.predict(test_feature)
print(prediction)
89/20:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbitbarkand'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['pi'])
prediction = regressor.predict(test_feature)
print(prediction)
89/21:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbitbarkand'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['wopi'])
prediction = regressor.predict(test_feature)
print(prediction)
89/22:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbitbarkand'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['woofpi'])
prediction = regressor.predict(test_feature)
print(prediction)
89/23:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbitbarkand'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['woof'])
prediction = regressor.predict(test_feature)
print(prediction)
89/24:
import pickle
pickl = {
    'vectorizer': vectorizer,
    'regressor': regressor
}
pickle.dump( pickl, open( 'models' + ".p", "wb" ) )
89/25:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['ribbitbarkand'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['boo'])
prediction = regressor.predict(test_feature)
print(prediction)
89/26:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['vahama'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['boo'])
prediction = regressor.predict(test_feature)
print(prediction)
89/27:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['bahama'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['boo'])
prediction = regressor.predict(test_feature)
print(prediction)
89/28:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['bow'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['boo'])
prediction = regressor.predict(test_feature)
print(prediction)
89/29:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['bowwow'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['boo'])
prediction = regressor.predict(test_feature)
print(prediction)
89/30:
test_feature = vectorizer.transform(['woof woof'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['wow'])
prediction = regressor.predict(test_feature)
print(prediction)
test_feature = vectorizer.transform(['boo'])
prediction = regressor.predict(test_feature)
print(prediction)
91/1: !pip install creme
91/2:
# Creating the pipeline
# 1st function is creating the bag of words
# 2nd function is the naive bayes predictor
model = compose.Pipeline(
    ('tokenize', feature_extraction.BagOfWords(lowercase=False)),
    ('nb', naive_bayes.MultinomialNB(alpha=1))
)
91/3:
from creme import compose
from creme import feature_extraction
from creme import naive_bayes
91/4:
from pprint import pprint
from creme import datasets
91/5: X_y = datasets.Phishing()  # this is a generator
91/6:
for x, y in X_y:
    pprint(x)
    print(y)
    break
91/7:
>>> from creme import compose
>>> from creme import linear_model
>>> from creme import metrics
>>> from creme import preprocessing

>>> model = compose.Pipeline(
...     preprocessing.StandardScaler(),
...     linear_model.LogisticRegression()
... )

>>> metric = metrics.Accuracy()

>>> for x, y in X_y:
...     y_pred = model.predict_one(x)      # make a prediction
...     metric = metric.update(y, y_pred)  # update the metric
...     model = model.fit_one(x, y)        # make the model learn

>>> metric
Accuracy: 89.20%
91/8:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()

for x, y in X_y:
    y_pred = model.predict_one(x)      # make a prediction
    metric = metric.update(y, y_pred)  # update the metric
    model = model.fit_one(x, y)        # make the model learn
91/9: metric
91/10:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()

for x, y in X_y:
    pprint(x, y)
    y_pred = model.predict_one(x)      # make a prediction
    metric = metric.update(y, y_pred)  # update the metric
    model = model.fit_one(x, y)        # make the model learn
91/11:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()

for x, y in X_y:
    pprint(x)
    y_pred = model.predict_one(x)      # make a prediction
    metric = metric.update(y, y_pred)  # update the metric
    model = model.fit_one(x, y)        # make the model learn
91/12:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()

for x, y in X_y:
    y_pred = model.predict_one(x)      # make a prediction
    metric = metric.update(y, y_pred)  # update the metric
    model = model.fit_one(x, y)        # make the model learn
91/13:
a = {'age_of_domain': 1,
 'anchor_from_other_domain': 0.0,
 'empty_server_form_handler': 0.0,
 'https': 0.0,
 'ip_in_url': 1,
 'is_popular': 0.5,
 'long_url': 1.0,
 'popup_window': 0.0,
 'request_from_other_domain': 0.0}
91/14: model.predic(a)
91/15: model.predict(a)
91/16: model.predict_one(a)
91/17:
i = 0
for x, y in X_y:
    pprint(x)
    print(y)
    if i == 5:
        break
    i += 1
91/18:
i = 0
for x, y in X_y:
    pprint(x)
    print(y)
    break
91/19:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()

#for x, y in X_y:
#    y_pred = model.predict_one(x)      # make a prediction
#    metric = metric.update(y, y_pred)  # update the metric
#    model = model.fit_one(x, y)        # make the model learn
91/20: model.predict_one(a)
92/1:
from pprint import pprint
from creme import datasets
92/2: X_y = datasets.Phishing()  # this is a generator
92/3:
i = 0
for x, y in X_y:
    pprint(x)
    print(y)
    break
92/4:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()

#for x, y in X_y:
#    y_pred = model.predict_one(x)      # make a prediction
#    metric = metric.update(y, y_pred)  # update the metric
#    model = model.fit_one(x, y)        # make the model learn
92/5:
a = {'age_of_domain': 1,
 'anchor_from_other_domain': 0.5,
 'empty_server_form_handler': 1.0,
 'https': 1.0,
 'ip_in_url': 1,
 'is_popular': 0.5,
 'long_url': 0.5,
 'popup_window': 0.5,
 'request_from_other_domain': 1.0}
92/6: model.predict_one(a)
92/7: model.predict_one(b)
92/8:
a = {'age_of_domain': 1,
 'anchor_from_other_domain': 0.5,
 'empty_server_form_handler': 1.0,
 'https': 1.0,
 'ip_in_url': 1,
 'is_popular': 0.5,
 'long_url': 0.5,
 'popup_window': 0.5,
 'request_from_other_domain': 1.0}

b = {'age_of_domain': 1,
 'anchor_from_other_domain': 0.0,
 'empty_server_form_handler': 0.0,
 'https': 0.0,
 'ip_in_url': 1,
 'is_popular': 0.5,
 'long_url': 1.0,
 'popup_window': 0.0,
 'request_from_other_domain': 0.0}
92/9: model.predict_one(a)
92/10: model.predict_one(b)
92/11:
y_pred = model.predict_one(a)
metric = metric.update(True, y_pred)
model = model.fit_one(x, True)
92/12: model.predict_one(b)
92/13:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()

#for x, y in X_y:
#    y_pred = model.predict_one(x)      # make a prediction
#    metric = metric.update(y, y_pred)  # update the metric
#    model = model.fit_one(x, y)        # make the model learn
92/14:
y_pred = model.predict_one(a)
metric = metric.update(False, y_pred)
model = model.fit_one(x, False)
92/15: model.predict_one(b)
92/16:
y_pred = model.predict_one(b)
metric = metric.update(True, y_pred)
model = model.fit_one(x, True)
92/17: model.predict_one(a)
92/18: model.predict_one(b)
92/19:
i = 0
for x, y in X_y:
    i += 1
    pprint(x)
    if i == 201:
        print(y)
        break
92/20:
i = 0
for x, y in X_y:
    i += 1
    if i == 201:
        pprint(x)
        print(y)
        break
92/21:
c = {'age_of_domain': 1,
 'anchor_from_other_domain': 0.5,
 'empty_server_form_handler': 1.0,
 'https': 1.0,
 'ip_in_url': 0,
 'is_popular': 0.0,
 'long_url': 0.5,
 'popup_window': 1.0,
 'request_from_other_domain': 1.0}
92/22: model.predict_one(c)
92/23:
i = 0
for x, y in X_y:
    i += 1
    if i == 202:
        pprint(x)
        print(y)
        break
92/24:
d = {'age_of_domain': 0,
 'anchor_from_other_domain': 1.0,
 'empty_server_form_handler': 0.5,
 'https': 0.0,
 'ip_in_url': 0,
 'is_popular': 0.5,
 'long_url': 0.0,
 'popup_window': 0.5,
 'request_from_other_domain': 0.5}
92/25: model.predict_one(d)
92/26:
y_pred = model.predict_one(c)
metric = metric.update(False, y_pred)
model = model.fit_one(x, False)
92/27: model.predict_one(d)
92/28:
y_pred = model.predict_one(d)
metric = metric.update(True, y_pred)
model = model.fit_one(x, True)
92/29:
i = 0
for x, y in X_y:
    i += 1
    if i == 203:
        pprint(x)
        print(y)
        break
92/30:
i = 0
for x, y in X_y:
    i += 1
    if i == 204:
        pprint(x)
        print(y)
        break
92/31:
i = 0
for x, y in X_y:
    i += 1
    if i == 205:
        pprint(x)
        print(y)
        break
92/32:
i = 0
for x, y in X_y:
    i += 1
    if i == 206:
        pprint(x)
        print(y)
        break
92/33:
e = {'age_of_domain': 1,
 'anchor_from_other_domain': 0.0,
 'empty_server_form_handler': 0.5,
 'https': 1.0,
 'ip_in_url': 0,
 'is_popular': 1.0,
 'long_url': 0.0,
 'popup_window': 0.0,
 'request_from_other_domain': 0.0}
92/34: model.predict_one(e)
92/35: cm = metrics.ConfusionMatrix()
92/36: cm
92/37:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()

for x, y in X_y:
    y_pred = model.predict_one(x)      # make a prediction
    metric = metric.update(y, y_pred)  # update the metric
    model = model.fit_one(x, y)        # make the model learn
92/38: cm = metrics.ConfusionMatrix()
92/39: cm
92/40: pprint(cm)
92/41: print(cm)
92/42:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()

for x, y in X_y:
    y_pred = model.predict_one(x)      # make a prediction
    metric = metric.update(y, y_pred)  # update the metric
    cm.update(y_pred=y_pred, y_true=y)
    model = model.fit_one(x, y)        # make the model learn
92/43: cm = metrics.ConfusionMatrix()
92/44: cm
92/45:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()
cm = metrics.ConfusionMatrix()

for x, y in X_y:
    y_pred = model.predict_one(x)      # make a prediction
    metric = metric.update(y, y_pred)  # update the metric
    cm.update(y_pred=y_pred, y_true=y)
    model = model.fit_one(x, y)        # make the model learn
92/46:
from creme import compose
from creme import linear_model
from creme import metrics
from creme import preprocessing

model = compose.Pipeline(
    preprocessing.StandardScaler(),
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()
cm = metrics.ConfusionMatrix()

for x, y in X_y:
    y_pred = model.predict_one(x)      # make a prediction
    metric = metric.update(y, y_pred)  # update the metric
    cm.update(y_pred=y_pred, y_true=y)
    model = model.fit_one(x, y)        # make the model learn
92/47: cm
93/1:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
   1:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
   2:
from articles.models import Article, Pathway
from django.db.models import Exists
   3:
has_pathway = Article.objects.all().exclude(annotations=None)
[entry for entry in has_pathway]
   4: print(has_pathway.first())
   5: print(has_pathway.first().annotationa)
   6: print(has_pathway.first().annotations)
   7: print(has_pathway[80].annotations)
   8: print(has_pathway[777].annotations)
   9: [entry for entry in has_pathway.exclude(annotations="[]")]
  10: has_pathway.filter(annotations!="[]")
  11: has_pathway.filter(article__annotations!="[]")
  12: has_pathway.objects.filter(annotations!="[]")
  13: has_pathway.filter(annotations!="[]")
  14: print(has_pathway)
  15: has_pathway.all().filter(annotations!="[]")
  16: print(has_pathway)
  17: print(has_pathway[888])
  18: print(has_pathway[888].annotations)
  19:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    for me in ar.annotations:
        print(me)
    i += 1
  20:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    annot = ar.annotations:
    print(json.loads(annot))
    i += 1
  21:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    annot = ar.annotations
    print(json.loads(annot))
    i += 1
  22:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
import json
  23:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    annot = ar.annotations
    print(json.loads(annot))
    i += 1
  24:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    annot = ar.annotations
    print(type(ar.annotations))
    i += 1
  25:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    annot = ar.annotations
    print(type(ar.annotations.length))
    i += 1
  26:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    annot = ar.annotations
    print(len(annot))
    i += 1
  27:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    annot = ar.annotations
    print(len(annot))
    if len(annot) > 1:
        print(annot)
        break
    i += 1
  28:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    annot = ar.annotations
    print(len(annot))
    if len(annot) > 1:
        print(annot)
        break
    i += 1
  29:
abstractText = "urea cycle has own idea"
nameOfPathway = "urea cycle"
for match in re.finditer(nameOfPathway, abstractText):
    print(match.start(),match.end(), "PATHWAY")
  30:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    annot = ar.annotations
    for an in annot:
        print(an)
    i += 1
  31:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot.tokens
        print(tokens)
    TRAIN_DATA.append((abstractText, {"entities": points}))
    for an in annot:
        print(an)
    i += 1
  32:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot.tokens
        print(tokens)
    i += 1
  33:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot.tokens
        print(json.load(tokens))
    i += 1
  34:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        print(json.load(tokens))
    i += 1
  35:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        print(tokens)
    i += 1
  36:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        print(tokens, start, end)
    i += 1
  37:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        print(tokens, start, end)
    print('------n')
    i += 1
  38:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        print(tokens, start, end)
    print('------\n')
    i += 1
  39:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
import json
from bs4 import BeautifulSoup
  40:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            print(BeautifulSoup(tok, "html.parser"))
        print(tokens, start, end)
    print('------\n')
    i += 1
  41:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            print(soup)
        print(tokens, start, end)
    print('------\n')
    i += 1
  42:
i = 0
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            print(soup)
        print(tokens, start, end)
    print('------\n')
    i += 1
  43:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            tok = tok.translate(table) 
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            print(soup)
        print(tokens, start, end)
    print('------\n')
    i += 1
  44:
import os
import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'gproject.settings')
os.environ["DJANGO_ALLOW_ASYNC_UNSAFE"] = "true"
django.setup()
import re
import json
from bs4 import BeautifulSoup
import string
  45:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            tok = tok.translate(table) 
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            print(soup)
        print(tokens, start, end)
    print('------\n')
    i += 1
  46:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup
            tok = tok.translate(table)
        print(tokens, start, end)
    print('------\n')
    i += 1
  47:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
        print(tokens, start, end)
    print('------\n')
    i += 1
  48:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            print(tok, soup)
        print(tokens, start, end)
    print('------\n')
    i += 1
  49:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            print(type(soup))
        print(tokens, start, end)
    print('------\n')
    i += 1
  50:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            print(type(soup.text))
        print(tokens, start, end)
    print('------\n')
    i += 1
  51:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
        print(tokens, start, end)
    print('------\n')
    i += 1
  52:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            print(tok)
        print(tokens, start, end)
    print('------\n')
    i += 1
  53:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            for match in re.finditer(tok, abstractText):
                print(match.start(), match.end())
            print(tok)
        print(tokens, start, end)
    print('------\n')
    i += 1
  54:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        words = []
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            pathway.append(tok)
        pathway = " ".join(words)
        for match in re.finditer(pathway, abstractText):
                print(match.start(), match.end())
        print(tokens, start, end)
    print('------\n')
    i += 1
  55:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        words = []
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            words.append(tok)
        pathway = " ".join(words)
        for match in re.finditer(pathway, abstractText):
                print(match.start(), match.end())
        print(tokens, start, end)
    print('------\n')
    i += 1
  56:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        words = []
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            words.append(tok)
        pathway = " ".join(words)
        for match in re.finditer(pathway, abstractText):
                points.append((match.start(),match.end(), "PATHWAY"))
        print(points)
        print(tokens, start, end)
    print('------\n')
    i += 1
  57:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        words = []
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            words.append(tok)
        pathway = " ".join(words)
        for match in re.finditer(pathway, abstractText):
            print(match.text)
            points.append((match.start(),match.end(), "PATHWAY"))
        print(points)
        print(tokens, start, end)
    print('------\n')
    i += 1
  58:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        words = []
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            words.append(tok)
        pathway = " ".join(words)
        for match in re.finditer(pathway, abstractText):
            print(match)
            points.append((match.start(),match.end(), "PATHWAY"))
        print(points)
        print(tokens, start, end)
    print('------\n')
    i += 1
  59:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        words = []
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            words.append(tok)
        pathway = " ".join(words)
        for match in re.finditer(pathway, abstractText):
            print(match.span)
            points.append((match.start(),match.end(), "PATHWAY"))
        print(points)
        print(tokens, start, end)
    print('------\n')
    i += 1
  60:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        words = []
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            words.append(tok)
        pathway = " ".join(words)
        for match in re.finditer(pathway, abstractText):
            print(match.span.text)
            points.append((match.start(),match.end(), "PATHWAY"))
        print(points)
        print(tokens, start, end)
    print('------\n')
    i += 1
  61:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        words = []
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            words.append(tok)
        pathway = " ".join(words)
        for match in re.finditer(pathway, abstractText):
            print(match)
            points.append((match.start(),match.end(), "PATHWAY"))
        print(points)
        print(tokens, start, end)
    print('------\n')
    i += 1
  62:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        words = []
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            words.append(tok)
        pathway = " ".join(words)
        for match in re.finditer(pathway, abstractText):
            print(match)
            points.append((match.start(),match.end(), "PATHWAY"))
        print(points)
        print(tokens, start, end)
    print(i)
    print('------\n')
    i += 1
  63:
i = 0
table = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}
for ar in has_pathway:
    abstractText = ar.abstract_text.lower()
    points = []
    for annot in ar.annotations:
        tokens = annot['tokens']
        start = annot['start']
        end = annot['end']
        words = []
        for tok in tokens:
            tok = tok.lower()
            soup = BeautifulSoup(tok, "html.parser")
            for elm in soup.find_all():
                elm.decompose()
            tok = soup.text
            tok = tok.translate(table)
            words.append(tok)
        pathway = " ".join(words)
        for match in re.finditer(pathway, abstractText):
            print(match)
            points.append((match.start(),match.end(), "PATHWAY"))
        print(points)
        print(tokens, start, end)
    print(i)
    print('------\n')
    if i == 5:
        print(abstractText)
        break
    i += 1
  64: %history -g -f filename
